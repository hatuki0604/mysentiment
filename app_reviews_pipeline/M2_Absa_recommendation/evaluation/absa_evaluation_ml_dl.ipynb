{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8653114",
   "metadata": {},
   "source": [
    "# üß™ ƒê√°nh gi√° c√°c ph∆∞∆°ng ph√°p ML/DL cho ABSA (UIT-ViSFD)\n",
    "\n",
    "Notebook n√†y ƒë√°nh gi√° c√°c ph∆∞∆°ng ph√°p **Machine Learning truy·ªÅn th·ªëng** v√† **Deep Learning** cho t√°c v·ª• **Aspect-Based Sentiment Analysis**.\n",
    "\n",
    "## C√°c ph∆∞∆°ng ph√°p ƒë∆∞·ª£c ƒë√°nh gi√°:\n",
    "\n",
    "### Machine Learning truy·ªÅn th·ªëng:\n",
    "- **Naive Bayes** (MultinomialNB)\n",
    "- **SVM** (Support Vector Machine v·ªõi kernel linear/rbf)\n",
    "- **Random Forest**\n",
    "\n",
    "### Deep Learning:\n",
    "- **CNN** (Convolutional Neural Network)\n",
    "- **LSTM** (Long Short-Term Memory)\n",
    "\n",
    "## Metrics ƒë√°nh gi√°:\n",
    "- Precision, Recall, F1-Score (macro, micro, weighted)\n",
    "- Confusion Matrix\n",
    "- Classification Report chi ti·∫øt theo t·ª´ng class\n",
    "\n",
    "> **L∆∞u √Ω**: Notebook n√†y t·∫≠p trung v√†o **Sentiment Classification** tr√™n c√°c kh√≠a c·∫°nh ƒë√£ ƒë∆∞·ª£c tr√≠ch xu·∫•t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312976e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 1) Thi·∫øt l·∫≠p ƒë∆∞·ªùng d·∫´n & tham s·ªë\n",
    "# ========================\n",
    "GROUND_TRUTH_CSV = \"/Users/hatrungkien/my-sentiment/data/raw/UIT-ViSFD/Test.csv\"\n",
    "PREDICTIONS_CSV = \"/Users/hatrungkien/my-sentiment/outputs/absa/openai/uit/uit_absa_2225.csv\"\n",
    "\n",
    "# B·ªè qua c√°c kh√≠a c·∫°nh t·ªïng qu√°t\n",
    "IGNORE_ASPECTS = {\"GENERAL\", \"OTHERS\"}\n",
    "\n",
    "# Random seed cho reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Tham s·ªë cho Deep Learning\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "EMBEDDING_DIM = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# Test size cho train/test split\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbf49af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 2) Import th∆∞ vi·ªán\n",
    "# ========================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Set, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# Traditional ML models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Deep Learning imports\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import (\n",
    "        Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout,\n",
    "        LSTM, Bidirectional, SpatialDropout1D\n",
    "    )\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    DL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è TensorFlow kh√¥ng ƒë∆∞·ª£c c√†i ƒë·∫∑t. C√°c m√¥ h√¨nh Deep Learning s·∫Ω b·ªã b·ªè qua.\")\n",
    "    DL_AVAILABLE = False\n",
    "\n",
    "print(\"‚úÖ Import th∆∞ vi·ªán th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 3) C√°c h√†m ti·ªán √≠ch\n",
    "# ========================\n",
    "\n",
    "def normalize_sentiment(s):\n",
    "    \"\"\"ƒê∆∞a sentiment v·ªÅ m·ªôt trong {Positive, Negative, Neutral}.\"\"\"\n",
    "    if s is None:\n",
    "        return None\n",
    "    s = str(s).strip().lower()\n",
    "    table = {\n",
    "        \"pos\": \"Positive\", \"positive\": \"Positive\", \"+\": \"Positive\",\n",
    "        \"neg\": \"Negative\", \"negative\": \"Negative\", \"-\": \"Negative\",\n",
    "        \"neu\": \"Neutral\",  \"neutral\":  \"Neutral\",  \"0\": \"Neutral\",\n",
    "    }\n",
    "    return table.get(s, s.capitalize())\n",
    "\n",
    "# B·∫£ng alias cho kh√≠a c·∫°nh\n",
    "ASPECT_ALIASES = {\n",
    "    \"screen\": \"SCREEN\", \"display\": \"SCREEN\", \"m√†n h√¨nh\": \"SCREEN\",\n",
    "    \"battery\": \"BATTERY\", \"pin\": \"BATTERY\",\n",
    "    \"camera\": \"CAMERA\", \"cam\": \"CAMERA\",\n",
    "    \"storage\": \"STORAGE\", \"memory\": \"STORAGE\", \"rom\": \"STORAGE\", \"ram\": \"STORAGE\",\n",
    "    \"design\": \"DESIGN\", \"thi·∫øt k·∫ø\": \"DESIGN\",\n",
    "    \"performance\": \"PERFORMANCE\", \"speed\": \"PERFORMANCE\", \"hi·ªáu nƒÉng\": \"PERFORMANCE\",\n",
    "    \"ser&acc\": \"SER&ACC\", \"service\": \"SER&ACC\", \"support\": \"SER&ACC\",\n",
    "    \"features\": \"FEATURES\", \"setup\": \"FEATURES\", \"t√≠nh nƒÉng\": \"FEATURES\",\n",
    "    \"sound\": \"SOUND\", \"audio\": \"SOUND\", \"loa\": \"SOUND\",\n",
    "    \"price\": \"PRICE\", \"gi√°\": \"PRICE\",\n",
    "    \"general\": \"GENERAL\", \"others\": \"OTHERS\", \"other\": \"OTHERS\"\n",
    "}\n",
    "\n",
    "def canonicalize_aspect(a):\n",
    "    \"\"\"Chu·∫©n h√≥a t√™n kh√≠a c·∫°nh.\"\"\"\n",
    "    if a is None:\n",
    "        return None\n",
    "    a0 = str(a).strip().strip('\"\\'').lower()\n",
    "    a0 = a0.replace(\"&amp;\", \"&\")\n",
    "    a0 = re.sub(r\"\\s+\", \" \", a0)\n",
    "    return ASPECT_ALIASES.get(a0, a0.upper())\n",
    "\n",
    "def parse_gt_labels(label_str: str) -> Dict[str, str]:\n",
    "    \"\"\"Parse ground truth labels t·ª´ ƒë·ªãnh d·∫°ng UIT-ViSFD.\"\"\"\n",
    "    result = {}\n",
    "    if pd.isna(label_str) or not str(label_str).strip():\n",
    "        return result\n",
    "    parts = str(label_str).split(\";\")\n",
    "    for item in parts:\n",
    "        item = item.strip().strip(\"{} \")\n",
    "        if not item:\n",
    "            continue\n",
    "        if \"#\" in item:\n",
    "            asp, sent = item.split(\"#\", 1)\n",
    "            asp = canonicalize_aspect(asp)\n",
    "            sent = normalize_sentiment(sent)\n",
    "        else:\n",
    "            asp = canonicalize_aspect(item)\n",
    "            sent = \"Neutral\"\n",
    "        if asp:\n",
    "            result[asp] = sent\n",
    "    return result\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"L√†m s·∫°ch vƒÉn b·∫£n ti·∫øng Vi·ªát.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower().strip()\n",
    "    # Gi·ªØ l·∫°i k√Ω t·ª± ti·∫øng Vi·ªát v√† s·ªë\n",
    "    text = re.sub(r'[^\\w\\s.,!?√†√°·∫£√£·∫°ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"‚úÖ ƒê·ªãnh nghƒ©a c√°c h√†m ti·ªán √≠ch th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 4) ƒê·ªçc v√† x·ª≠ l√Ω d·ªØ li·ªáu\n",
    "# ========================\n",
    "\n",
    "# ƒê·ªçc ground truth\n",
    "gt = pd.read_csv(GROUND_TRUTH_CSV)\n",
    "print(f\"Ground truth shape: {gt.shape}\")\n",
    "print(f\"Columns: {gt.columns.tolist()}\")\n",
    "\n",
    "# T√¨m c·ªôt ID v√† c·ªôt text\n",
    "id_col = 'index' if 'index' in gt.columns else gt.columns[0]\n",
    "text_col = 'comment' if 'comment' in gt.columns else 'sentence'\n",
    "label_col = 'label' if 'label' in gt.columns else gt.columns[-1]\n",
    "\n",
    "gt['review_id'] = gt[id_col]\n",
    "gt['text'] = gt[text_col]\n",
    "\n",
    "print(f\"\\nS·ªë l∆∞·ª£ng reviews: {len(gt)}\")\n",
    "display(gt.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 5) T·∫°o dataset cho training\n",
    "# ========================\n",
    "\n",
    "# T·∫°o d·ªØ li·ªáu ·ªü c·∫•p (text, aspect, sentiment)\n",
    "data_records = []\n",
    "\n",
    "for idx, row in gt.iterrows():\n",
    "    text = clean_text(row['text'])\n",
    "    if not text:\n",
    "        continue\n",
    "    \n",
    "    labels = parse_gt_labels(row[label_col])\n",
    "    \n",
    "    for aspect, sentiment in labels.items():\n",
    "        if aspect in IGNORE_ASPECTS:\n",
    "            continue\n",
    "        if sentiment not in ['Positive', 'Negative', 'Neutral']:\n",
    "            continue\n",
    "        \n",
    "        # T·∫°o text k·∫øt h·ª£p aspect\n",
    "        combined_text = f\"{text} [ASPECT: {aspect}]\"\n",
    "        \n",
    "        data_records.append({\n",
    "            'review_id': row['review_id'],\n",
    "            'text': text,\n",
    "            'aspect': aspect,\n",
    "            'combined_text': combined_text,\n",
    "            'sentiment': sentiment\n",
    "        })\n",
    "\n",
    "df_data = pd.DataFrame(data_records)\n",
    "print(f\"T·ªïng s·ªë samples: {len(df_data)}\")\n",
    "print(f\"\\nPh√¢n b·ªë sentiment:\")\n",
    "print(df_data['sentiment'].value_counts())\n",
    "print(f\"\\nPh√¢n b·ªë aspect:\")\n",
    "print(df_data['aspect'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 6) Chia train/test v√† chu·∫©n b·ªã features\n",
    "# ========================\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df_data['sentiment_encoded'] = label_encoder.fit_transform(df_data['sentiment'])\n",
    "\n",
    "print(f\"Label mapping: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")\n",
    "\n",
    "# Chia train/test\n",
    "X_text = df_data['combined_text'].values\n",
    "y = df_data['sentiment_encoded'].values\n",
    "\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X_text, y, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain size: {len(X_train_text)}\")\n",
    "print(f\"Test size: {len(X_test_text)}\")\n",
    "\n",
    "# TF-IDF Vectorizer cho ML truy·ªÅn th·ªëng\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_text)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_text)\n",
    "\n",
    "print(f\"\\nTF-IDF features shape: {X_train_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluation_functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 7) H√†m ƒë√°nh gi√° v√† hi·ªÉn th·ªã k·∫øt qu·∫£\n",
    "# ========================\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name, label_names=None):\n",
    "    \"\"\"ƒê√°nh gi√° v√† hi·ªÉn th·ªã k·∫øt qu·∫£ c·ªßa model.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä K·∫øt qu·∫£: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"\\nüéØ Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # Precision, Recall, F1\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro'\n",
    "    )\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='micro'\n",
    "    )\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüìà Metrics (Macro):\")\n",
    "    print(f\"   Precision: {precision_macro:.4f}\")\n",
    "    print(f\"   Recall:    {recall_macro:.4f}\")\n",
    "    print(f\"   F1-Score:  {f1_macro:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüìà Metrics (Micro):\")\n",
    "    print(f\"   Precision: {precision_micro:.4f}\")\n",
    "    print(f\"   Recall:    {recall_micro:.4f}\")\n",
    "    print(f\"   F1-Score:  {f1_micro:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüìà Metrics (Weighted):\")\n",
    "    print(f\"   Precision: {precision_weighted:.4f}\")\n",
    "    print(f\"   Recall:    {recall_weighted:.4f}\")\n",
    "    print(f\"   F1-Score:  {f1_weighted:.4f}\")\n",
    "    \n",
    "    # Classification Report\n",
    "    print(f\"\\nüìã Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=label_names))\n",
    "    \n",
    "    # Return metrics dict\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'accuracy': acc,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'precision_micro': precision_micro,\n",
    "        'recall_micro': recall_micro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'precision_weighted': precision_weighted,\n",
    "        'recall_weighted': recall_weighted,\n",
    "        'f1_weighted': f1_weighted\n",
    "    }\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, label_names, model_name):\n",
    "    \"\"\"V·∫Ω confusion matrix.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ ƒê·ªãnh nghƒ©a c√°c h√†m ƒë√°nh gi√° th√†nh c√¥ng!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ml_models_header",
   "metadata": {},
   "source": [
    "---\n",
    "# ü§ñ Ph·∫ßn 1: Machine Learning Truy·ªÅn Th·ªëng\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naive_bayes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 8) Naive Bayes\n",
    "# ========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîπ Training Naive Bayes (MultinomialNB)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "nb_model = MultinomialNB(alpha=1.0)\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate\n",
    "nb_metrics = evaluate_model(\n",
    "    y_test, y_pred_nb, \n",
    "    \"Naive Bayes (MultinomialNB)\",\n",
    "    label_encoder.classes_\n",
    ")\n",
    "\n",
    "# Confusion Matrix\n",
    "plot_confusion_matrix(y_test, y_pred_nb, label_encoder.classes_, \"Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "svm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 9) Support Vector Machine (SVM)\n",
    "# ========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîπ Training SVM (Linear Kernel)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "svm_model = SVC(\n",
    "    kernel='linear',\n",
    "    C=1.0,\n",
    "    random_state=RANDOM_STATE,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate\n",
    "svm_metrics = evaluate_model(\n",
    "    y_test, y_pred_svm,\n",
    "    \"SVM (Linear Kernel)\",\n",
    "    label_encoder.classes_\n",
    ")\n",
    "\n",
    "# Confusion Matrix\n",
    "plot_confusion_matrix(y_test, y_pred_svm, label_encoder.classes_, \"SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random_forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 10) Random Forest\n",
    "# ========================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üîπ Training Random Forest...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_rf = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate\n",
    "rf_metrics = evaluate_model(\n",
    "    y_test, y_pred_rf,\n",
    "    \"Random Forest\",\n",
    "    label_encoder.classes_\n",
    ")\n",
    "\n",
    "# Confusion Matrix\n",
    "plot_confusion_matrix(y_test, y_pred_rf, label_encoder.classes_, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dl_models_header",
   "metadata": {},
   "source": [
    "---\n",
    "# üß† Ph·∫ßn 2: Deep Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare_dl_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 11) Chu·∫©n b·ªã d·ªØ li·ªáu cho Deep Learning\n",
    "# ========================\n",
    "\n",
    "if DL_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîß Chu·∫©n b·ªã d·ªØ li·ªáu cho Deep Learning...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Tokenizer\n",
    "    tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(X_train_text)\n",
    "    \n",
    "    # Convert to sequences\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test_text)\n",
    "    \n",
    "    # Pad sequences\n",
    "    X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "    X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "    \n",
    "    # One-hot encode labels\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    y_train_cat = to_categorical(y_train, num_classes)\n",
    "    y_test_cat = to_categorical(y_test, num_classes)\n",
    "    \n",
    "    vocab_size = min(10000, len(tokenizer.word_index) + 1)\n",
    "    \n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "    print(f\"Max sequence length: {MAX_SEQUENCE_LENGTH}\")\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "    print(f\"X_train_pad shape: {X_train_pad.shape}\")\n",
    "    print(f\"X_test_pad shape: {X_test_pad.shape}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è TensorFlow kh√¥ng kh·∫£ d·ª•ng. B·ªè qua Deep Learning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cnn_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 12) CNN Model\n",
    "# ========================\n",
    "\n",
    "if DL_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîπ Training CNN (Convolutional Neural Network)...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Build CNN model\n",
    "    cnn_model = Sequential([\n",
    "        Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
    "        SpatialDropout1D(0.2),\n",
    "        Conv1D(128, 5, activation='relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    cnn_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(cnn_model.summary())\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history_cnn = cnn_model.fit(\n",
    "        X_train_pad, y_train_cat,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_cnn_proba = cnn_model.predict(X_test_pad)\n",
    "    y_pred_cnn = np.argmax(y_pred_cnn_proba, axis=1)\n",
    "    \n",
    "    # Evaluate\n",
    "    cnn_metrics = evaluate_model(\n",
    "        y_test, y_pred_cnn,\n",
    "        \"CNN\",\n",
    "        label_encoder.classes_\n",
    "    )\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plot_confusion_matrix(y_test, y_pred_cnn, label_encoder.classes_, \"CNN\")\n",
    "    \n",
    "    # Plot training history\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    axes[0].plot(history_cnn.history['accuracy'], label='Train Accuracy')\n",
    "    axes[0].plot(history_cnn.history['val_accuracy'], label='Val Accuracy')\n",
    "    axes[0].set_title('CNN - Accuracy')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].plot(history_cnn.history['loss'], label='Train Loss')\n",
    "    axes[1].plot(history_cnn.history['val_loss'], label='Val Loss')\n",
    "    axes[1].set_title('CNN - Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    cnn_metrics = None\n",
    "    print(\"‚ö†Ô∏è B·ªè qua CNN do TensorFlow kh√¥ng kh·∫£ d·ª•ng.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lstm_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 13) LSTM Model\n",
    "# ========================\n",
    "\n",
    "if DL_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîπ Training LSTM (Long Short-Term Memory)...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Build LSTM model\n",
    "    lstm_model = Sequential([\n",
    "        Embedding(vocab_size, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH),\n",
    "        SpatialDropout1D(0.2),\n",
    "        Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    lstm_model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(lstm_model.summary())\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history_lstm = lstm_model.fit(\n",
    "        X_train_pad, y_train_cat,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_lstm_proba = lstm_model.predict(X_test_pad)\n",
    "    y_pred_lstm = np.argmax(y_pred_lstm_proba, axis=1)\n",
    "    \n",
    "    # Evaluate\n",
    "    lstm_metrics = evaluate_model(\n",
    "        y_test, y_pred_lstm,\n",
    "        \"LSTM (Bidirectional)\",\n",
    "        label_encoder.classes_\n",
    "    )\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    plot_confusion_matrix(y_test, y_pred_lstm, label_encoder.classes_, \"LSTM\")\n",
    "    \n",
    "    # Plot training history\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    axes[0].plot(history_lstm.history['accuracy'], label='Train Accuracy')\n",
    "    axes[0].plot(history_lstm.history['val_accuracy'], label='Val Accuracy')\n",
    "    axes[0].set_title('LSTM - Accuracy')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].plot(history_lstm.history['loss'], label='Train Loss')\n",
    "    axes[1].plot(history_lstm.history['val_loss'], label='Val Loss')\n",
    "    axes[1].set_title('LSTM - Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    lstm_metrics = None\n",
    "    print(\"‚ö†Ô∏è B·ªè qua LSTM do TensorFlow kh√¥ng kh·∫£ d·ª•ng.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison_header",
   "metadata": {},
   "source": [
    "---\n",
    "# üìä Ph·∫ßn 3: So s√°nh t·ªïng h·ª£p c√°c ph∆∞∆°ng ph√°p\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 14) T·ªïng h·ª£p v√† so s√°nh k·∫øt qu·∫£\n",
    "# ========================\n",
    "\n",
    "# Collect all metrics\n",
    "all_metrics = [nb_metrics, svm_metrics, rf_metrics]\n",
    "if DL_AVAILABLE and cnn_metrics:\n",
    "    all_metrics.append(cnn_metrics)\n",
    "if DL_AVAILABLE and lstm_metrics:\n",
    "    all_metrics.append(lstm_metrics)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame(all_metrics)\n",
    "comparison_df = comparison_df.set_index('model')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä B·∫¢NG SO S√ÅNH T·ªîNG H·ª¢P C√ÅC PH∆Ø∆†NG PH√ÅP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display formatted\n",
    "display_cols = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro', \n",
    "                'precision_weighted', 'recall_weighted', 'f1_weighted']\n",
    "display(comparison_df[display_cols].round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 15) Visualize comparison\n",
    "# ========================\n",
    "\n",
    "# Plot F1 scores comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# F1 Macro comparison\n",
    "models = comparison_df.index.tolist()\n",
    "f1_macro = comparison_df['f1_macro'].values\n",
    "f1_weighted = comparison_df['f1_weighted'].values\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[0].bar(x - width/2, f1_macro, width, label='F1 Macro')\n",
    "bars2 = axes[0].bar(x + width/2, f1_weighted, width, label='F1 Weighted')\n",
    "\n",
    "axes[0].set_xlabel('Model')\n",
    "axes[0].set_ylabel('F1 Score')\n",
    "axes[0].set_title('So s√°nh F1 Score gi·ªØa c√°c ph∆∞∆°ng ph√°p')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    axes[0].annotate(f'{height:.3f}',\n",
    "                    xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    axes[0].annotate(f'{height:.3f}',\n",
    "                    xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Accuracy comparison\n",
    "accuracy = comparison_df['accuracy'].values\n",
    "bars3 = axes[1].bar(models, accuracy, color='steelblue')\n",
    "axes[1].set_xlabel('Model')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('So s√°nh Accuracy gi·ªØa c√°c ph∆∞∆°ng ph√°p')\n",
    "axes[1].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "for bar in bars3:\n",
    "    height = bar.get_height()\n",
    "    axes[1].annotate(f'{height:.3f}',\n",
    "                    xy=(bar.get_x() + bar.get_width()/2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radar_chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 16) Radar Chart so s√°nh\n",
    "# ========================\n",
    "\n",
    "from math import pi\n",
    "\n",
    "# Ch·ªçn metrics ƒë·ªÉ so s√°nh\n",
    "categories = ['Accuracy', 'Precision\\n(Macro)', 'Recall\\n(Macro)', 'F1\\n(Macro)']\n",
    "N = len(categories)\n",
    "\n",
    "# T·∫°o angles\n",
    "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(comparison_df)))\n",
    "\n",
    "for idx, (model_name, row) in enumerate(comparison_df.iterrows()):\n",
    "    values = [row['accuracy'], row['precision_macro'], \n",
    "              row['recall_macro'], row['f1_macro']]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[idx])\n",
    "    ax.fill(angles, values, alpha=0.1, color=colors[idx])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "ax.set_title('So s√°nh hi·ªáu nƒÉng c√°c ph∆∞∆°ng ph√°p\\n(Radar Chart)', size=14, y=1.08)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 17) L∆∞u k·∫øt qu·∫£\n",
    "# ========================\n",
    "from pathlib import Path\n",
    "\n",
    "out_dir = Path(\"absa_ml_dl_eval_reports\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df.to_csv(out_dir / \"model_comparison.csv\")\n",
    "\n",
    "# Save detailed results as JSON\n",
    "results_summary = {\n",
    "    'config': {\n",
    "        'ground_truth_csv': GROUND_TRUTH_CSV,\n",
    "        'predictions_csv': PREDICTIONS_CSV,\n",
    "        'ignore_aspects': list(IGNORE_ASPECTS),\n",
    "        'test_size': TEST_SIZE,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'max_sequence_length': MAX_SEQUENCE_LENGTH,\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'epochs': EPOCHS\n",
    "    },\n",
    "    'results': all_metrics,\n",
    "    'best_model': comparison_df['f1_macro'].idxmax(),\n",
    "    'best_f1_macro': float(comparison_df['f1_macro'].max())\n",
    "}\n",
    "\n",
    "with open(out_dir / \"results_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results_summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o th∆∞ m·ª•c: {out_dir}\")\n",
    "print(f\"   - model_comparison.csv\")\n",
    "print(f\"   - results_summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# 18) K·∫øt lu·∫≠n\n",
    "# ========================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìã K·∫æT LU·∫¨N\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_model = comparison_df['f1_macro'].idxmax()\n",
    "best_f1 = comparison_df.loc[best_model, 'f1_macro']\n",
    "best_acc = comparison_df.loc[best_model, 'accuracy']\n",
    "\n",
    "print(f\"\\nüèÜ M√¥ h√¨nh t·ªët nh·∫•t (theo F1 Macro): {best_model}\")\n",
    "print(f\"   - F1 Macro:  {best_f1:.4f}\")\n",
    "print(f\"   - Accuracy:  {best_acc:.4f}\")\n",
    "\n",
    "print(\"\\nüìä X·∫øp h·∫°ng c√°c m√¥ h√¨nh (theo F1 Macro):\")\n",
    "ranking = comparison_df['f1_macro'].sort_values(ascending=False)\n",
    "for i, (model, score) in enumerate(ranking.items(), 1):\n",
    "    print(f\"   {i}. {model}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìù GHI CH√ö:\")\n",
    "print(\"- K·∫øt qu·∫£ c√≥ th·ªÉ thay ƒë·ªïi t√πy thu·ªôc v√†o tham s·ªë v√† random seed.\")\n",
    "print(\"- Deep Learning models c·∫ßn nhi·ªÅu d·ªØ li·ªáu h∆°n ƒë·ªÉ ƒë·∫°t hi·ªáu qu·∫£ t·ªët.\")\n",
    "print(\"- C√≥ th·ªÉ c·∫£i thi·ªán b·∫±ng: hyperparameter tuning, pre-trained embeddings, ensemble.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appendix",
   "metadata": {},
   "source": [
    "---\n",
    "## üìö Ph·ª• l·ª•c: G·ª£i √Ω c·∫£i thi·ªán\n",
    "\n",
    "### 1. Hyperparameter Tuning\n",
    "- S·ª≠ d·ª•ng GridSearchCV ho·∫∑c RandomizedSearchCV cho ML models\n",
    "- Keras Tuner cho Deep Learning models\n",
    "\n",
    "### 2. Pre-trained Embeddings\n",
    "- Word2Vec ti·∫øng Vi·ªát\n",
    "- FastText ti·∫øng Vi·ªát\n",
    "- PhoBERT (transformer-based)\n",
    "\n",
    "### 3. Ensemble Methods\n",
    "- Voting Classifier\n",
    "- Stacking\n",
    "- Bagging\n",
    "\n",
    "### 4. Advanced Deep Learning\n",
    "- Attention mechanisms\n",
    "- Transformer-based models (BERT, PhoBERT)\n",
    "- Multi-task learning\n",
    "\n",
    "### 5. Data Augmentation\n",
    "- Back-translation\n",
    "- Synonym replacement\n",
    "- Random insertion/deletion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
