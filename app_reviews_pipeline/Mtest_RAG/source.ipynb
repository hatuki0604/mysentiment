{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4512a3a",
   "metadata": {},
   "source": [
    "## convert data vá» Ä‘Ãºng cá»™t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d0668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"Datasets.csv\")\n",
    "\n",
    "# TÃ¡ch product vÃ  review\n",
    "def split_sentence(s):\n",
    "    if \" - \" in s:\n",
    "        product, review = s.split(\" - \", 1)\n",
    "        return product.strip(), review.strip()\n",
    "    return s.strip(), \"\"   # fallback náº¿u khÃ´ng cÃ³ dáº¥u '-'\n",
    "\n",
    "df[\"product\"], df[\"review\"] = zip(*df[\"sentence\"].apply(split_sentence))\n",
    "\n",
    "# Náº¿u muá»‘n bá» prefix nhÆ° \"Äiá»‡n thoáº¡i \"\n",
    "def normalize_product(p):\n",
    "    prefixes = [\"Äiá»‡n thoáº¡i \", \"Smartphone \", \"MÃ¡y tÃ­nh báº£ng \"]\n",
    "    for pre in prefixes:\n",
    "        if p.startswith(pre):\n",
    "            return p[len(pre):].strip()\n",
    "    return p\n",
    "\n",
    "df[\"product\"] = df[\"product\"].apply(normalize_product)\n",
    "\n",
    "df.to_csv(\"clean_reviews.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fd269be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸš€ SIMPLE STATISTICS RAG SYSTEM\n",
      "================================================================================\n",
      "ðŸ“‚ Loading data...\n",
      "âœ… Loaded 2429 reviews\n",
      "ðŸ“± Products: 2322\n",
      "âœ… System ready!\n",
      "\n",
      "================================================================================\n",
      "âœ… READY TO USE!\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ Example usage:\n",
      "answer = rag.answer('Pin Xiaomi 15T cÃ³ tá»‘t khÃ´ng?')\n",
      "answer = rag.answer('So sÃ¡nh camera Xiaomi 15T vÃ  15T Pro')\n",
      "answer = rag.answer('ÄÃ¡nh giÃ¡ chung vá» Xiaomi 15T Pro')\n",
      "================================================================================\n",
      "ðŸš€ SIMPLE STATISTICS RAG SYSTEM\n",
      "================================================================================\n",
      "ðŸ“‚ Loading data...\n",
      "âœ… Loaded 2429 reviews\n",
      "ðŸ“± Products: 2322\n",
      "âœ… System ready!\n",
      "\n",
      "================================================================================\n",
      "âœ… READY TO USE!\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ Example usage:\n",
      "answer = rag.answer('Pin Xiaomi 15T cÃ³ tá»‘t khÃ´ng?')\n",
      "answer = rag.answer('So sÃ¡nh camera Xiaomi 15T vÃ  15T Pro')\n",
      "answer = rag.answer('ÄÃ¡nh giÃ¡ chung vá» Xiaomi 15T Pro')\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ® INTERACTIVE DEMO\n",
      "================================================================================\n",
      "Type 'exit' to quit\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "â“ Query: iphone 17 pro pin tháº¿ nÃ o\n",
      "================================================================================\n",
      "\n",
      "ðŸ” Step 1: Parsing query...\n",
      "ðŸ” Parsed: {'products': ['iphone 17 pro'], 'aspects': ['battery'], 'sentiment_focus': 'null', 'is_comparison': False}\n",
      "\n",
      "ðŸ“Š Step 2: Filtering data...\n",
      "âš ï¸ No products matched query\n",
      "\n",
      "ðŸ¤– Answer:\n",
      "âš ï¸ KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u phÃ¹ há»£p vá»›i cÃ¢u há»i cá»§a báº¡n.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "â“ Query: iphone 17 pro\n",
      "================================================================================\n",
      "\n",
      "ðŸ” Step 1: Parsing query...\n",
      "ðŸ” Parsed: {'products': ['iphone 17 pro'], 'aspects': ['general'], 'sentiment_focus': 'null', 'is_comparison': False}\n",
      "\n",
      "ðŸ“Š Step 2: Filtering data...\n",
      "âš ï¸ No products matched query\n",
      "\n",
      "ðŸ¤– Answer:\n",
      "âš ï¸ KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u phÃ¹ há»£p vá»›i cÃ¢u há»i cá»§a báº¡n.\n",
      "\n",
      "ðŸ‘‹ Goodbye!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple Statistics-Based RAG System\n",
    "- LLM parses query to extract product + aspects\n",
    "- Filter data using fuzzy matching\n",
    "- Compute statistics on filtered data\n",
    "- LLM generates final answer\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from collections import Counter\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ============================================================================\n",
    "# MODULE 1: DATA LOADER\n",
    "# ============================================================================\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Load and prepare data\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path: str):\n",
    "        self.csv_path = csv_path\n",
    "        self.df = None\n",
    "    \n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load CSV\"\"\"\n",
    "        print(\"ðŸ“‚ Loading data...\")\n",
    "        self.df = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Parse aspects and sentiments\n",
    "        self.df['aspects'] = self.df['aspects'].apply(self._safe_parse)\n",
    "        self.df['sentiments'] = self.df['sentiments'].apply(self._safe_parse)\n",
    "        \n",
    "        print(f\"âœ… Loaded {len(self.df)} reviews\")\n",
    "        print(f\"ðŸ“± Products: {self.df['product'].nunique()}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def _safe_parse(self, x):\n",
    "        \"\"\"Parse string to list/dict\"\"\"\n",
    "        if pd.isna(x):\n",
    "            return [] if '[' in str(x) else {}\n",
    "        if isinstance(x, (list, dict)):\n",
    "            return x\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except:\n",
    "            return [] if '[' in str(x) else {}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODULE 2: QUERY PARSER (LLM)\n",
    "# ============================================================================\n",
    "\n",
    "class QueryParser:\n",
    "    \"\"\"Use LLM to parse user query\"\"\"\n",
    "    \n",
    "    def __init__(self, client: OpenAI, available_products: List[str]):\n",
    "        self.client = client\n",
    "        self.available_products = available_products\n",
    "    \n",
    "    def parse_query(self, query: str) -> Dict:\n",
    "        \"\"\"Parse query to extract intent, products, aspects\"\"\"\n",
    "        \n",
    "        system_prompt = f\"\"\"Báº¡n lÃ  trá»£ lÃ½ phÃ¢n tÃ­ch cÃ¢u há»i vá» Ä‘Ã¡nh giÃ¡ Ä‘iá»‡n thoáº¡i.\n",
    "\n",
    "Nhiá»‡m vá»¥: PhÃ¢n tÃ­ch cÃ¢u há»i Ä‘á»ƒ trÃ­ch xuáº¥t:\n",
    "1. products: Danh sÃ¡ch tÃªn sáº£n pháº©m (cÃ³ thá»ƒ viáº¿t táº¯t/khÃ´ng chÃ­nh xÃ¡c)\n",
    "2. aspects: Danh sÃ¡ch khÃ­a cáº¡nh Ä‘Æ°á»£c há»i (general, battery, camera, performance, screen, design, price, storage, features, ser&acc)\n",
    "3. sentiment_focus: NgÆ°á»i dÃ¹ng muá»‘n biáº¿t Æ°u Ä‘iá»ƒm (positive), nhÆ°á»£c Ä‘iá»ƒm (negative), hay tá»•ng quan (null)\n",
    "4. is_comparison: CÃ³ pháº£i cÃ¢u há»i so sÃ¡nh khÃ´ng? (true/false)\n",
    "\n",
    "ASPECTS cÃ³ thá»ƒ cÃ³:\n",
    "- general: Ä‘Ã¡nh giÃ¡ chung\n",
    "- battery/pin: pin\n",
    "- camera: camera\n",
    "- performance: hiá»‡u nÄƒng, tá»‘c Ä‘á»™, chip\n",
    "- screen: mÃ n hÃ¬nh\n",
    "- design: thiáº¿t káº¿, ngoáº¡i hÃ¬nh\n",
    "- price: giÃ¡ cáº£\n",
    "- storage: bá»™ nhá»›, lÆ°u trá»¯\n",
    "- features: tÃ­nh nÄƒng\n",
    "- ser&acc: dá»‹ch vá»¥, phá»¥ kiá»‡n\n",
    "\n",
    "Danh sÃ¡ch sáº£n pháº©m cÃ³ sáºµn (Ä‘á»ƒ tham kháº£o):\n",
    "{', '.join(self.available_products[:10])}...\n",
    "\n",
    "Tráº£ vá» JSON vá»›i format:\n",
    "{{\n",
    "  \"products\": [\"product_name1\", \"product_name2\"],\n",
    "  \"aspects\": [\"aspect1\", \"aspect2\"],\n",
    "  \"sentiment_focus\": \"positive|negative|null\",\n",
    "  \"is_comparison\": true|false\n",
    "}}\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"CÃ¢u há»i: {query}\\n\\nPhÃ¢n tÃ­ch:\"}\n",
    "                ],\n",
    "                temperature=0.1,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            parsed = json.loads(response.choices[0].message.content)\n",
    "            print(f\"ðŸ” Parsed: {parsed}\")\n",
    "            return parsed\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Parse error: {e}\")\n",
    "            return {\n",
    "                \"products\": [],\n",
    "                \"aspects\": [\"general\"],\n",
    "                \"sentiment_focus\": None,\n",
    "                \"is_comparison\": False\n",
    "            }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODULE 3: DATA FILTER\n",
    "# ============================================================================\n",
    "\n",
    "class DataFilter:\n",
    "    \"\"\"Filter data based on parsed query\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "        self.all_products = df['product'].unique().tolist()\n",
    "    \n",
    "    def keyword_match_product(self, query_product: str) -> List[str]:\n",
    "        \"\"\"Match products using keyword extraction with exact priority\"\"\"\n",
    "        query_lower = query_product.lower().strip()\n",
    "        \n",
    "        # Extract keywords from query (split and clean)\n",
    "        keywords = []\n",
    "        for word in query_lower.split():\n",
    "            # Remove common words and keep meaningful parts\n",
    "            if len(word) >= 2 and word not in ['Ä‘iá»‡n', 'thoáº¡i', 'mÃ¡y', 'chiáº¿c', 'cÃ¡i', 'gb', 'tb']:\n",
    "                keywords.append(word)\n",
    "        \n",
    "        if not keywords:\n",
    "            return []\n",
    "        \n",
    "        exact_matches = []\n",
    "        partial_matches = []\n",
    "        \n",
    "        for product in self.all_products:\n",
    "            product_lower = product.lower()\n",
    "            \n",
    "            # Strategy 1: Exact phrase match (highest priority)\n",
    "            # Check if query is a substring of product name\n",
    "            if query_lower in product_lower:\n",
    "                # But make sure it's a word boundary match\n",
    "                # e.g., \"iphone 17\" should not match \"iphone 170\"\n",
    "                import re\n",
    "                # Add word boundaries\n",
    "                pattern = r'\\b' + re.escape(query_lower) + r'\\b'\n",
    "                if re.search(pattern, product_lower):\n",
    "                    exact_matches.append(product)\n",
    "                    continue\n",
    "            \n",
    "            # Strategy 2: All keywords must exist (partial match)\n",
    "            if all(kw in product_lower for kw in keywords):\n",
    "                # Additional check: avoid over-matching\n",
    "                # If query is \"iphone 17\", don't match \"iphone 17 pro\"\n",
    "                # unless query also has \"pro\"\n",
    "                \n",
    "                # Extract product keywords (significant words)\n",
    "                product_words = set(re.findall(r'\\b\\w+\\b', product_lower))\n",
    "                query_words = set(keywords)\n",
    "                \n",
    "                # If product has extra significant model keywords not in query, skip\n",
    "                significant_extras = ['pro', 'max', 'plus', 'ultra', 'mini', 'lite', 'note']\n",
    "                extra_in_product = product_words.intersection(significant_extras)\n",
    "                extra_in_query = query_words.intersection(significant_extras)\n",
    "                \n",
    "                # If product has model variant keywords that query doesn't have, skip it\n",
    "                if extra_in_product - extra_in_query:\n",
    "                    continue\n",
    "                \n",
    "                partial_matches.append(product)\n",
    "        \n",
    "        # Return exact matches first, then partial if no exact\n",
    "        if exact_matches:\n",
    "            return exact_matches\n",
    "        return partial_matches\n",
    "    \n",
    "    def filter_data(self, products: List[str], aspects: List[str]) -> tuple:\n",
    "        \"\"\"Filter dataframe based on products and aspects\"\"\"\n",
    "        \n",
    "        # Step 1: Match products using keywords\n",
    "        matched_products = []\n",
    "        \n",
    "        if not products:\n",
    "            # No product specified, use all\n",
    "            matched_products = self.all_products\n",
    "        else:\n",
    "            for query_prod in products:\n",
    "                matched = self.keyword_match_product(query_prod)\n",
    "                matched_products.extend(matched)\n",
    "            \n",
    "            matched_products = list(set(matched_products))\n",
    "        \n",
    "        if not matched_products:\n",
    "            print(\"âš ï¸ No products matched query\")\n",
    "            return pd.DataFrame(), [], {}\n",
    "            # matched_products = self.all_products\n",
    "        else:\n",
    "            print(f\"âœ… Matched {len(matched_products)} products: {matched_products[:3]}{'...' if len(matched_products) > 3 else ''}\")\n",
    "        \n",
    "        # Filter by products FIRST (don't filter by aspect yet for counting)\n",
    "        product_filtered = self.df[self.df['product'].isin(matched_products)].copy()\n",
    "        \n",
    "        # Count total reviews per product BEFORE aspect filtering\n",
    "        product_review_counts = {}\n",
    "        for product in matched_products:\n",
    "            product_df = product_filtered[product_filtered['product'] == product]\n",
    "            # Use review_id if available, otherwise count unique reviews\n",
    "            if 'review_id' in product_df.columns:\n",
    "                product_review_counts[product] = product_df['review_id'].nunique()\n",
    "            else:\n",
    "                product_review_counts[product] = product_df['review'].nunique()\n",
    "        \n",
    "        # Step 2: Filter by aspects for retrieval (but keep counts from above)\n",
    "        if aspects and aspects != ['general']:\n",
    "            def has_aspect_match(review_aspects):\n",
    "                if not isinstance(review_aspects, list):\n",
    "                    return False\n",
    "                return any(asp in aspects for asp in review_aspects)\n",
    "            \n",
    "            aspect_filtered = product_filtered[product_filtered['aspects'].apply(has_aspect_match)].copy()\n",
    "        else:\n",
    "            aspect_filtered = product_filtered.copy()\n",
    "        \n",
    "        print(f\"ðŸ“Š Filtered to {len(aspect_filtered)} review entries\")\n",
    "        \n",
    "        return aspect_filtered, matched_products, product_review_counts\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODULE 4: STATISTICS COMPUTER\n",
    "# ============================================================================\n",
    "\n",
    "class StatisticsComputer:\n",
    "    \"\"\"Compute statistics on filtered data\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_stats(df: pd.DataFrame, aspects: List[str], product_review_counts: Dict[str, int]) -> Dict:\n",
    "        \"\"\"Compute comprehensive statistics\n",
    "        \n",
    "        Args:\n",
    "            df: Filtered dataframe (may be filtered by aspect)\n",
    "            aspects: List of aspects to analyze\n",
    "            product_review_counts: Dict of {product: total_review_count} BEFORE aspect filtering\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            return {\n",
    "                'total_reviews': 0,\n",
    "                'message': 'KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u phÃ¹ há»£p'\n",
    "            }\n",
    "        \n",
    "        # Total reviews across all matched products (from pre-computed counts)\n",
    "        total_reviews = sum(product_review_counts.values())\n",
    "        \n",
    "        stats = {\n",
    "            'total_reviews': total_reviews,\n",
    "            'products': {},\n",
    "            'aspects': {},\n",
    "            'overall_sentiment': {}\n",
    "        }\n",
    "        \n",
    "        # Overall sentiment distribution (count from filtered df, but calculate % based on total)\n",
    "        all_sentiments = []\n",
    "        for sentiments_dict in df['sentiments']:\n",
    "            if isinstance(sentiments_dict, dict):\n",
    "                all_sentiments.extend(sentiments_dict.values())\n",
    "        \n",
    "        sentiment_counts = Counter(all_sentiments)\n",
    "        \n",
    "        stats['overall_sentiment'] = {\n",
    "            'positive': sentiment_counts.get('Positive', 0),\n",
    "            'neutral': sentiment_counts.get('Neutral', 0),\n",
    "            'negative': sentiment_counts.get('Negative', 0),\n",
    "            'positive_pct': sentiment_counts.get('Positive', 0) / total_reviews * 100 if total_reviews > 0 else 0,\n",
    "            'negative_pct': sentiment_counts.get('Negative', 0) / total_reviews * 100 if total_reviews > 0 else 0,\n",
    "            'neutral_pct': sentiment_counts.get('Neutral', 0) / total_reviews * 100 if total_reviews > 0 else 0\n",
    "        }\n",
    "        \n",
    "        # Per-product stats\n",
    "        for product, product_total_reviews in product_review_counts.items():\n",
    "            product_df = df[df['product'] == product]\n",
    "            stats['products'][product] = StatisticsComputer._compute_product_stats(\n",
    "                product_df, \n",
    "                product_total_reviews\n",
    "            )\n",
    "        \n",
    "        # Per-aspect stats\n",
    "        for aspect in aspects:\n",
    "            aspect_reviews = []\n",
    "            aspect_sentiments = []\n",
    "            \n",
    "            for idx, row in df.iterrows():\n",
    "                if isinstance(row['aspects'], list) and aspect in row['aspects']:\n",
    "                    aspect_reviews.append(row['sentence'])\n",
    "                    if isinstance(row['sentiments'], dict) and aspect in row['sentiments']:\n",
    "                        aspect_sentiments.append(row['sentiments'][aspect])\n",
    "            \n",
    "            sentiment_counts = Counter(aspect_sentiments)\n",
    "            positive_count = sentiment_counts.get('Positive', 0)\n",
    "            negative_count = sentiment_counts.get('Negative', 0)\n",
    "            neutral_count = sentiment_counts.get('Neutral', 0)\n",
    "            \n",
    "            # Calculate based on TOTAL reviews\n",
    "            implicit_neutral = total_reviews - len(aspect_reviews)\n",
    "            total_neutral = neutral_count + implicit_neutral\n",
    "            \n",
    "            stats['aspects'][aspect] = {\n",
    "                'count': len(aspect_reviews),\n",
    "                'positive': positive_count,\n",
    "                'neutral': total_neutral,\n",
    "                'negative': negative_count,\n",
    "                'positive_pct': positive_count / total_reviews * 100 if total_reviews > 0 else 0,\n",
    "                'negative_pct': negative_count / total_reviews * 100 if total_reviews > 0 else 0,\n",
    "                'neutral_pct': total_neutral / total_reviews * 100 if total_reviews > 0 else 0,\n",
    "                'mentioned_pct': len(aspect_reviews) / total_reviews * 100 if total_reviews > 0 else 0,\n",
    "                'sample_reviews': aspect_reviews[:3]\n",
    "            }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    @staticmethod\n",
    "    def _compute_product_stats(product_df: pd.DataFrame, total_reviews: int) -> Dict:\n",
    "        \"\"\"Compute stats for a single product\n",
    "        \n",
    "        Args:\n",
    "            product_df: Filtered dataframe for this product (may be aspect-filtered)\n",
    "            total_reviews: ACTUAL total reviews for this product (before aspect filtering)\n",
    "        \"\"\"\n",
    "        all_sentiments = []\n",
    "        for sentiments_dict in product_df['sentiments']:\n",
    "            if isinstance(sentiments_dict, dict):\n",
    "                all_sentiments.extend(sentiments_dict.values())\n",
    "        \n",
    "        sentiment_counts = Counter(all_sentiments)\n",
    "        \n",
    "        return {\n",
    "            'review_count': total_reviews,\n",
    "            'positive': sentiment_counts.get('Positive', 0),\n",
    "            'neutral': sentiment_counts.get('Neutral', 0),\n",
    "            'negative': sentiment_counts.get('Negative', 0),\n",
    "            'positive_pct': sentiment_counts.get('Positive', 0) / total_reviews * 100 if total_reviews > 0 else 0,\n",
    "            'negative_pct': sentiment_counts.get('Negative', 0) / total_reviews * 100 if total_reviews > 0 else 0,\n",
    "            'neutral_pct': sentiment_counts.get('Neutral', 0) / total_reviews * 100 if total_reviews > 0 else 0\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODULE 5: CONTEXT BUILDER\n",
    "# ============================================================================\n",
    "\n",
    "class ContextBuilder:\n",
    "    \"\"\"Build context for LLM from statistics\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_context(stats: Dict, df: pd.DataFrame, parsed_query: Dict) -> str:\n",
    "        \"\"\"Build comprehensive context\"\"\"\n",
    "        \n",
    "        if stats.get('total_reviews', 0) == 0:\n",
    "            return \"KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ phÃ¹ há»£p vá»›i cÃ¢u há»i.\"\n",
    "        \n",
    "        parts = []\n",
    "        \n",
    "        # Header\n",
    "        parts.append(f\"=== Tá»”NG QUAN ===\")\n",
    "        parts.append(f\"Tá»•ng sá»‘ Ä‘Ã¡nh giÃ¡: {stats['total_reviews']}\")\n",
    "        \n",
    "        # Overall sentiment\n",
    "        if 'overall_sentiment' in stats:\n",
    "            s = stats['overall_sentiment']\n",
    "            parts.append(f\"\\nPhÃ¢n bá»‘ cáº£m xÃºc tá»•ng thá»ƒ:\")\n",
    "            parts.append(f\"  â€¢ TÃ­ch cá»±c: {s['positive_pct']:.1f}% ({s['positive']} Ä‘Ã¡nh giÃ¡)\")\n",
    "            parts.append(f\"  â€¢ Trung láº­p: {s['neutral_pct']:.1f}% ({s['neutral']} Ä‘Ã¡nh giÃ¡)\")\n",
    "            parts.append(f\"  â€¢ TiÃªu cá»±c: {s['negative_pct']:.1f}% ({s['negative']} Ä‘Ã¡nh giÃ¡)\")\n",
    "        \n",
    "        # Per-product breakdown\n",
    "        if len(stats['products']) > 1 or parsed_query.get('is_comparison'):\n",
    "            parts.append(f\"\\n=== SO SÃNH Sáº¢N PHáº¨M ===\")\n",
    "            for product, pstats in stats['products'].items():\n",
    "                parts.append(f\"\\n{product}:\")\n",
    "                parts.append(f\"  â€¢ Sá»‘ Ä‘Ã¡nh giÃ¡: {pstats['review_count']}\")\n",
    "                parts.append(f\"  â€¢ TÃ­ch cá»±c: {pstats['positive_pct']:.1f}% ({pstats['positive']})\")\n",
    "                parts.append(f\"  â€¢ TiÃªu cá»±c: {pstats['negative_pct']:.1f}% ({pstats['negative']})\")\n",
    "        \n",
    "        # Per-aspect breakdown\n",
    "        if stats['aspects']:\n",
    "            parts.append(f\"\\n=== PHÃ‚N TÃCH THEO KHÃA Cáº NH ===\")\n",
    "            for aspect, astats in stats['aspects'].items():\n",
    "                parts.append(f\"\\n{aspect.upper()}:\")\n",
    "                parts.append(f\"  â€¢ Sá»‘ Ä‘Ã¡nh giÃ¡ Ä‘á» cáº­p: {astats['count']}/{stats['total_reviews']} ({astats['mentioned_pct']:.1f}%)\")\n",
    "                parts.append(f\"  â€¢ TÃ­ch cá»±c: {astats['positive_pct']:.1f}% ({astats['positive']} Ä‘Ã¡nh giÃ¡)\")\n",
    "                parts.append(f\"  â€¢ TiÃªu cá»±c: {astats['negative_pct']:.1f}% ({astats['negative']} Ä‘Ã¡nh giÃ¡)\")\n",
    "                parts.append(f\"  â€¢ Trung láº­p/KhÃ´ng Ä‘á» cáº­p: {astats['neutral_pct']:.1f}% ({astats['neutral']} Ä‘Ã¡nh giÃ¡)\")\n",
    "                \n",
    "                if astats['sample_reviews']:\n",
    "                    parts.append(f\"  â€¢ VÃ­ dá»¥:\")\n",
    "                    for i, review in enumerate(astats['sample_reviews'][:2], 1):\n",
    "                        parts.append(f\"    {i}. {review[:150]}\")\n",
    "        \n",
    "        # Sample reviews\n",
    "        parts.append(f\"\\n=== MáºªU ÄÃNH GIÃ ===\")\n",
    "        sample_df = df.head(5)\n",
    "        for i, row in enumerate(sample_df.iterrows(), 1):\n",
    "            idx, r = row\n",
    "            parts.append(f\"\\n{i}. [{r['product']}]\")\n",
    "            parts.append(f\"   KhÃ­a cáº¡nh: {', '.join(r['aspects']) if isinstance(r['aspects'], list) else 'N/A'}\")\n",
    "            parts.append(f\"   Ná»™i dung: {r['sentence'][:200]}\")\n",
    "        \n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODULE 6: RAG SYSTEM\n",
    "# ============================================================================\n",
    "\n",
    "class SimpleRAG:\n",
    "    \"\"\"Main RAG system\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path: str, openai_api_key: Optional[str] = None):\n",
    "        # Load API key\n",
    "        api_key = openai_api_key or os.getenv('OPENAI_API_KEY')\n",
    "        if not api_key:\n",
    "            raise ValueError(\"âŒ OpenAI API key not found! Add to .env file\")\n",
    "        \n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        \n",
    "        # Load data\n",
    "        loader = DataLoader(csv_path)\n",
    "        self.df = loader.load_data()\n",
    "        \n",
    "        # Initialize modules\n",
    "        self.parser = QueryParser(self.client, self.df['product'].unique().tolist())\n",
    "        self.filter = DataFilter(self.df)\n",
    "        \n",
    "        print(\"âœ… System ready!\")\n",
    "    \n",
    "    def answer(self, query: str) -> str:\n",
    "        \"\"\"Main method: answer user query\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"â“ Query: {query}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Step 1: Parse query with LLM\n",
    "        print(\"\\nðŸ” Step 1: Parsing query...\")\n",
    "        parsed = self.parser.parse_query(query)\n",
    "        \n",
    "        # Step 2: Filter data\n",
    "        print(\"\\nðŸ“Š Step 2: Filtering data...\")\n",
    "        filtered_df, matched_products, product_review_counts = self.filter.filter_data(\n",
    "            parsed.get('products', []),\n",
    "            parsed.get('aspects', ['general'])\n",
    "        )\n",
    "        \n",
    "        if len(filtered_df) == 0:\n",
    "            return \"âš ï¸ KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u phÃ¹ há»£p vá»›i cÃ¢u há»i cá»§a báº¡n.\"\n",
    "        \n",
    "        # Step 3: Compute statistics\n",
    "        print(\"\\nðŸ“ˆ Step 3: Computing statistics...\")\n",
    "        stats = StatisticsComputer.compute_stats(\n",
    "            filtered_df,\n",
    "            parsed.get('aspects', ['general']),\n",
    "            product_review_counts\n",
    "        )\n",
    "        \n",
    "        # Step 4: Build context\n",
    "        print(\"\\nðŸ“ Step 4: Building context...\")\n",
    "        context = ContextBuilder.build_context(stats, filtered_df, parsed)\n",
    "        \n",
    "        # Step 5: Generate answer with LLM\n",
    "        print(\"\\nðŸ¤– Step 5: Generating answer...\")\n",
    "        answer = self._generate_answer(query, context, parsed)\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def _generate_answer(self, query: str, context: str, parsed_query: Dict) -> str:\n",
    "        \"\"\"Generate final answer using LLM\"\"\"\n",
    "        \n",
    "        system_prompt = \"\"\"Báº¡n lÃ  trá»£ lÃ½ phÃ¢n tÃ­ch Ä‘Ã¡nh giÃ¡ Ä‘iá»‡n thoáº¡i chuyÃªn nghiá»‡p.\n",
    "\n",
    "NHIá»†M Vá»¤:\n",
    "Dá»±a trÃªn dá»¯ liá»‡u thá»‘ng kÃª Ä‘Æ°á»£c cung cáº¥p, hÃ£y tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng má»™t cÃ¡ch:\n",
    "- ChÃ­nh xÃ¡c, dá»±a trÃªn sá»‘ liá»‡u cá»¥ thá»ƒ\n",
    "- CÃ¢n báº±ng, khÃ´ng thiÃªn vá»‹\n",
    "- Dá»… hiá»ƒu, sÃºc tÃ­ch\n",
    "- TrÃ­ch dáº«n % vÃ  sá»‘ lÆ°á»£ng review\n",
    "\n",
    "Cáº¤U TRÃšC TRáº¢ Lá»œI:\n",
    "1. TÃ³m táº¯t ngáº¯n gá»n (1-2 cÃ¢u)\n",
    "2. PhÃ¢n tÃ­ch sá»‘ liá»‡u chi tiáº¿t\n",
    "3. ÄÆ°a ra vÃ­ dá»¥ tá»« review (náº¿u cÃ³)\n",
    "4. Káº¿t luáº­n\n",
    "\n",
    "LÆ¯U Ã:\n",
    "- KHÃ”NG bá»‹a Ä‘áº·t thÃ´ng tin ngoÃ i context\n",
    "- Náº¿u dá»¯ liá»‡u khÃ´ng Ä‘á»§, nÃ³i rÃµ háº¡n cháº¿\n",
    "- Æ¯u tiÃªn sá»‘ liá»‡u thá»‘ng kÃª hÆ¡n review Ä‘Æ¡n láº»\"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"Dá»¯ liá»‡u thá»‘ng kÃª:\n",
    "{context}\n",
    "\n",
    "CÃ¢u há»i: {query}\n",
    "\n",
    "HÃ£y tráº£ lá»i dá»±a trÃªn dá»¯ liá»‡u trÃªn:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "            \n",
    "            answer = response.choices[0].message.content\n",
    "            \n",
    "            # Log token usage\n",
    "            usage = response.usage\n",
    "            print(f\"ðŸ’° Tokens: {usage.total_tokens} (prompt: {usage.prompt_tokens}, completion: {usage.completion_tokens})\")\n",
    "            \n",
    "            return answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"âš ï¸ Lá»—i khi generate answer: {e}\\n\\nContext:\\n{context}\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the system\"\"\"\n",
    "    \n",
    "    CSV_PATH = \"clean_reviews.csv\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ðŸš€ SIMPLE STATISTICS RAG SYSTEM\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize RAG\n",
    "    rag = SimpleRAG(CSV_PATH)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… READY TO USE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nðŸ“ Example usage:\")\n",
    "    print(\"answer = rag.answer('Pin Xiaomi 15T cÃ³ tá»‘t khÃ´ng?')\")\n",
    "    print(\"answer = rag.answer('So sÃ¡nh camera Xiaomi 15T vÃ  15T Pro')\")\n",
    "    print(\"answer = rag.answer('ÄÃ¡nh giÃ¡ chung vá» Xiaomi 15T Pro')\")\n",
    "    \n",
    "    return rag\n",
    "\n",
    "\n",
    "def demo():\n",
    "    \"\"\"Interactive demo\"\"\"\n",
    "    rag = main()\n",
    "        \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸŽ® INTERACTIVE DEMO\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Type 'exit' to quit\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\nðŸ’¬ Your question: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['exit', 'quit', 'q']:\n",
    "                print(\"ðŸ‘‹ Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            answer = rag.answer(user_input)\n",
    "            print(f\"\\nðŸ¤– Answer:\\n{answer}\\n\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nðŸ‘‹ Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâš ï¸ Error: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run setup\n",
    "    rag = main()\n",
    "    \n",
    "    # Uncomment to run interactive demo\n",
    "    demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "761e3903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸš€ SIMPLE STATISTICS RAG SYSTEM WITH VISUALIZATION\n",
      "================================================================================\n",
      "ðŸ“‚ Loading data...\n",
      "âœ… Loaded 2429 reviews\n",
      "ðŸ“± Products: 195\n",
      "âœ… System ready!\n",
      "\n",
      "================================================================================\n",
      "âœ… READY TO USE!\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ Example usage:\n",
      "result = rag.answer('Pin Xiaomi 15T cÃ³ tá»‘t khÃ´ng?')\n",
      "print(result['answer'])\n",
      "# Save charts:\n",
      "for chart_name, img_base64 in result['charts'].items():\n",
      "    with open(f'{chart_name}.png', 'wb') as f:\n",
      "        f.write(base64.b64decode(img_base64))\n",
      "\n",
      "================================================================================\n",
      "ðŸ§ª RUNNING EXAMPLE QUERY\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "â“ Query: So sÃ¡nh pin Xiaomi 15T vÃ  15T Pro\n",
      "================================================================================\n",
      "\n",
      "ðŸ” Step 1: Parsing query...\n",
      "ðŸ” Parsed: {'products': ['Xiaomi 15T', 'Xiaomi 15T Pro'], 'aspects': ['battery'], 'sentiment_focus': None, 'is_comparison': True}\n",
      "\n",
      "ðŸ“Š Step 2: Filtering data...\n",
      "âœ… Matched 2 products: ['Xiaomi 15T 5G 12GB/256GB', 'Xiaomi 15T Pro 5G 12GB/256GB']\n",
      "ðŸ“Š Filtered to 12 review entries\n",
      "\n",
      "ðŸ“ˆ Step 3: Computing statistics...\n",
      "\n",
      "ðŸ“ Step 4: Building context...\n",
      "\n",
      "ðŸ“Š Step 5: Generating charts...\n",
      "âœ… Generated 3 chart(s)\n",
      "\n",
      "ðŸ¤– Step 6: Generating answer...\n",
      "ðŸ’° Tokens: 1408 (prompt: 1016, completion: 392)\n",
      "\n",
      "ðŸ¤– Answer:\n",
      "1. **TÃ³m táº¯t ngáº¯n gá»n**: Pin cá»§a Xiaomi 15T vÃ  15T Pro cÃ³ sá»± khÃ¡c biá»‡t rÃµ rá»‡t trong pháº£n há»“i cá»§a ngÆ°á»i dÃ¹ng, vá»›i Xiaomi 15T nháº­n nhiá»u Ä‘Ã¡nh giÃ¡ tiÃªu cá»±c hÆ¡n vá» hiá»‡u suáº¥t pin so vá»›i 15T Pro.\n",
      "\n",
      "2. **PhÃ¢n tÃ­ch sá»‘ liá»‡u chi tiáº¿t**: Trong tá»•ng sá»‘ 28 Ä‘Ã¡nh giÃ¡, 12 Ä‘Ã¡nh giÃ¡ (42.9%) Ä‘á» cáº­p Ä‘áº¿n pin. Trong Ä‘Ã³, 8 Ä‘Ã¡nh giÃ¡ (28.6%) vá» pin lÃ  tiÃªu cá»±c, khÃ´ng cÃ³ Ä‘Ã¡nh giÃ¡ nÃ o tÃ­ch cá»±c. Äá»‘i vá»›i Xiaomi 15T 5G, trong sá»‘ 22 Ä‘Ã¡nh giÃ¡, cÃ³ nhiá»u pháº£n há»“i tiÃªu cá»±c vá» pin, nhÆ° \"mÃ¡y má»›i mua 2 ngÃ y mÃ  tuá»™t pin quÃ¡\" vÃ  \"Ä‘á»ƒ qua Ä‘Ãªm sao tuá»™t táº§m 15% pin\". NgÆ°á»£c láº¡i, Xiaomi 15T Pro 5G nháº­n Ä‘Æ°á»£c 100% Ä‘Ã¡nh giÃ¡ tÃ­ch cá»±c vá» pin tá»« 6 Ä‘Ã¡nh giÃ¡, vá»›i ngÆ°á»i dÃ¹ng cho ráº±ng pin \"thá»«a sá»©c Ä‘Ã¡p á»©ng 1 ngÃ y dÃ i\".\n",
      "\n",
      "3. **ÄÆ°a ra vÃ­ dá»¥ tá»« review**: Má»™t ngÆ°á»i dÃ¹ng nháº­n xÃ©t vá» Xiaomi 15T Pro ráº±ng \"pin khÃ´ng trÃ¢u láº¯m nhÆ°ng thá»«a sá»©c Ä‘Ã¡p á»©ng 1 ngÃ y dÃ i lÃ m viá»‡c nhiá»u\", trong khi má»™t ngÆ°á»i dÃ¹ng khÃ¡c vá» Xiaomi 15T cho biáº¿t \"mÃ¡y má»›i mua 2 ngÃ y mÃ  tuá»™t pin quÃ¡\".\n",
      "\n",
      "4. **Káº¿t luáº­n**: Tá»« dá»¯ liá»‡u thá»‘ng kÃª, cÃ³ thá»ƒ tháº¥y ráº±ng Xiaomi 15T Pro 5G vÆ°á»£t trá»™i hÆ¡n vá» hiá»‡u suáº¥t pin so vá»›i Xiaomi 15T 5G, vá»›i 100% Ä‘Ã¡nh giÃ¡ tÃ­ch cá»±c cho 15T Pro vÃ  nhiá»u pháº£n há»“i tiÃªu cá»±c cho 15T. Äiá»u nÃ y cho tháº¥y ngÆ°á»i dÃ¹ng cÃ³ xu hÆ°á»›ng hÃ i lÃ²ng hÆ¡n vá»›i kháº£ nÄƒng sá»­ dá»¥ng pin cá»§a 15T Pro.\n",
      "\n",
      "ðŸ“„ HTML report saved: review_analysis.html\n",
      "ðŸ’¡ Open review_analysis.html in your browser to view charts!\n",
      "================================================================================\n",
      "ðŸš€ SIMPLE STATISTICS RAG SYSTEM WITH VISUALIZATION\n",
      "================================================================================\n",
      "ðŸ“‚ Loading data...\n",
      "âœ… Loaded 2429 reviews\n",
      "ðŸ“± Products: 195\n",
      "âœ… System ready!\n",
      "\n",
      "================================================================================\n",
      "âœ… READY TO USE!\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ Example usage:\n",
      "result = rag.answer('Pin Xiaomi 15T cÃ³ tá»‘t khÃ´ng?')\n",
      "print(result['answer'])\n",
      "# Save charts:\n",
      "for chart_name, img_base64 in result['charts'].items():\n",
      "    with open(f'{chart_name}.png', 'wb') as f:\n",
      "        f.write(base64.b64decode(img_base64))\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ® INTERACTIVE DEMO\n",
      "================================================================================\n",
      "Type 'exit' to quit\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "================================================================================\n",
      "â“ Query: Xiaomi 15T pin cÃ³ tá»‘t khÃ´ng\n",
      "================================================================================\n",
      "\n",
      "ðŸ” Step 1: Parsing query...\n",
      "ðŸ” Parsed: {'products': ['Xiaomi 15T'], 'aspects': ['battery'], 'sentiment_focus': 'null', 'is_comparison': False}\n",
      "\n",
      "ðŸ“Š Step 2: Filtering data...\n",
      "âœ… Matched 2 products: ['Xiaomi 15T 5G 12GB/256GB', 'Xiaomi 15T Pro 5G 12GB/256GB']\n",
      "ðŸ“Š Filtered to 12 review entries\n",
      "\n",
      "ðŸ“ˆ Step 3: Computing statistics...\n",
      "\n",
      "ðŸ“ Step 4: Building context...\n",
      "\n",
      "ðŸ“Š Step 5: Generating charts...\n",
      "âœ… Generated 3 chart(s)\n",
      "\n",
      "ðŸ¤– Step 6: Generating answer...\n",
      "ðŸ’° Tokens: 1350 (prompt: 1011, completion: 339)\n",
      "\n",
      "ðŸ¤– Answer:\n",
      "1. TÃ³m táº¯t ngáº¯n gá»n:\n",
      "Pin cá»§a Xiaomi 15T khÃ´ng Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ cao, vá»›i 28.6% Ä‘Ã¡nh giÃ¡ tiÃªu cá»±c liÃªn quan Ä‘áº¿n hiá»‡u suáº¥t pin vÃ  khÃ´ng cÃ³ Ä‘Ã¡nh giÃ¡ tÃ­ch cá»±c nÃ o vá» khÃ­a cáº¡nh nÃ y.\n",
      "\n",
      "2. PhÃ¢n tÃ­ch sá»‘ liá»‡u chi tiáº¿t:\n",
      "Trong tá»•ng sá»‘ 28 Ä‘Ã¡nh giÃ¡, cÃ³ 12 Ä‘Ã¡nh giÃ¡ Ä‘á» cáº­p Ä‘áº¿n pin, trong Ä‘Ã³ khÃ´ng cÃ³ Ä‘Ã¡nh giÃ¡ nÃ o Ä‘Æ°á»£c xem lÃ  tÃ­ch cá»±c (0.0%). 28.6% (8 Ä‘Ã¡nh giÃ¡) cho ráº±ng pin khÃ´ng tá»‘t, trong khi 71.4% (20 Ä‘Ã¡nh giÃ¡) khÃ´ng Ä‘á» cáº­p Ä‘áº¿n váº¥n Ä‘á» nÃ y. So vá»›i tá»•ng thá»ƒ, 35.7% Ä‘Ã¡nh giÃ¡ cá»§a Xiaomi 15T lÃ  tiÃªu cá»±c, cho tháº¥y má»™t tá»· lá»‡ Ä‘Ã¡ng ká»ƒ ngÆ°á»i dÃ¹ng khÃ´ng hÃ i lÃ²ng vá»›i sáº£n pháº©m.\n",
      "\n",
      "3. ÄÆ°a ra vÃ­ dá»¥ tá»« review:\n",
      "Má»™t sá»‘ Ä‘Ã¡nh giÃ¡ cá»¥ thá»ƒ vá» pin cá»§a Xiaomi 15T cho tháº¥y sá»± khÃ´ng hÃ i lÃ²ng: \"MÃ¡y má»›i mua 2 ngÃ y mÃ  tuá»™t pin quÃ¡\" vÃ  \"MÃ¡y Ä‘á»ƒ qua Ä‘Ãªm sao tuá»™t táº§m 15% pin\". Äiá»u nÃ y cho tháº¥y ráº±ng nhiá»u ngÆ°á»i dÃ¹ng gáº·p váº¥n Ä‘á» vá»›i thá»i gian sá»­ dá»¥ng pin.\n",
      "\n",
      "4. Káº¿t luáº­n:\n",
      "Dá»±a trÃªn dá»¯ liá»‡u thá»‘ng kÃª, pin cá»§a Xiaomi 15T khÃ´ng Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ tá»‘t, vá»›i 28.6% Ä‘Ã¡nh giÃ¡ tiÃªu cá»±c vÃ  khÃ´ng cÃ³ Ä‘Ã¡nh giÃ¡ tÃ­ch cá»±c nÃ o vá» hiá»‡u suáº¥t pin. NgÆ°á»i dÃ¹ng cÃ³ váº» gáº·p khÃ³ khÄƒn trong viá»‡c duy trÃ¬ thá»i gian sá»­ dá»¥ng pin, Ä‘iá»u nÃ y cÃ³ thá»ƒ lÃ  má»™t yáº¿u tá»‘ cáº§n cÃ¢n nháº¯c khi quyáº¿t Ä‘á»‹nh mua sáº£n pháº©m nÃ y.\n",
      "\n",
      "ðŸ“Š Generated 3 chart(s):\n",
      "  âœ… Saved: sentiment_pie.png\n",
      "  âœ… Saved: product_comparison.png\n",
      "  âœ… Saved: aspect_breakdown.png\n",
      "\n",
      "ðŸ’¡ Tip: Open the PNG files to view charts!\n",
      "\n",
      "ðŸ‘‹ Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODULE 6: CONTEXT BUILDER\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "Simple Statistics-Based RAG System\n",
    "- LLM parses query to extract product + aspects\n",
    "- Filter data using fuzzy matching\n",
    "- Compute statistics on filtered data\n",
    "- LLM generates final answer\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from collections import Counter\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import BytesIO\n",
    "import base64\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set style for better looking plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "\n",
    "# ============================================================================\n",
    "# MODULE 1: DATA LOADER\n",
    "# ============================================================================\n",
    "\n",
    "class DataLoader:\n",
    "    \"\"\"Load and prepare data\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path: str):\n",
    "        self.csv_path = csv_path\n",
    "        self.df = None\n",
    "    \n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load CSV\"\"\"\n",
    "        print(\"ðŸ“‚ Loading data...\")\n",
    "        self.df = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Parse aspects and sentiments\n",
    "        self.df['aspects'] = self.df['aspects'].apply(self._safe_parse)\n",
    "        self.df['sentiments'] = self.df['sentiments'].apply(self._safe_parse)\n",
    "        \n",
    "        print(f\"âœ… Loaded {len(self.df)} reviews\")\n",
    "        print(f\"ðŸ“± Products: {self.df['product'].nunique()}\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def _safe_parse(self, x):\n",
    "        \"\"\"Parse string to list/dict\"\"\"\n",
    "        if pd.isna(x):\n",
    "            return [] if '[' in str(x) else {}\n",
    "        if isinstance(x, (list, dict)):\n",
    "            return x\n",
    "        try:\n",
    "            return ast.literal_eval(x)\n",
    "        except:\n",
    "            return [] if '[' in str(x) else {}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODULE 2: QUERY PARSER (LLM)\n",
    "# ============================================================================\n",
    "\n",
    "class QueryParser:\n",
    "    \"\"\"Use LLM to parse user query\"\"\"\n",
    "    \n",
    "    def __init__(self, client: OpenAI, available_products: List[str]):\n",
    "        self.client = client\n",
    "        self.available_products = available_products\n",
    "    \n",
    "    def parse_query(self, query: str) -> Dict:\n",
    "        \"\"\"Parse query to extract intent, products, aspects\"\"\"\n",
    "        \n",
    "        system_prompt = f\"\"\"Báº¡n lÃ  trá»£ lÃ½ phÃ¢n tÃ­ch cÃ¢u há»i vá» Ä‘Ã¡nh giÃ¡ Ä‘iá»‡n thoáº¡i.\n",
    "\n",
    "Nhiá»‡m vá»¥: PhÃ¢n tÃ­ch cÃ¢u há»i Ä‘á»ƒ trÃ­ch xuáº¥t:\n",
    "1. products: Danh sÃ¡ch tÃªn sáº£n pháº©m (cÃ³ thá»ƒ viáº¿t táº¯t/khÃ´ng chÃ­nh xÃ¡c)\n",
    "2. aspects: Danh sÃ¡ch khÃ­a cáº¡nh Ä‘Æ°á»£c há»i (general, battery, camera, performance, screen, design, price, storage, features, ser&acc)\n",
    "3. sentiment_focus: NgÆ°á»i dÃ¹ng muá»‘n biáº¿t Æ°u Ä‘iá»ƒm (positive), nhÆ°á»£c Ä‘iá»ƒm (negative), hay tá»•ng quan (null)\n",
    "4. is_comparison: CÃ³ pháº£i cÃ¢u há»i so sÃ¡nh khÃ´ng? (true/false)\n",
    "\n",
    "ASPECTS cÃ³ thá»ƒ cÃ³:\n",
    "- general: Ä‘Ã¡nh giÃ¡ chung\n",
    "- battery/pin: pin\n",
    "- camera: camera\n",
    "- performance: hiá»‡u nÄƒng, tá»‘c Ä‘á»™, chip\n",
    "- screen: mÃ n hÃ¬nh\n",
    "- design: thiáº¿t káº¿, ngoáº¡i hÃ¬nh\n",
    "- price: giÃ¡ cáº£\n",
    "- storage: bá»™ nhá»›, lÆ°u trá»¯\n",
    "- features: tÃ­nh nÄƒng\n",
    "- ser&acc: dá»‹ch vá»¥, phá»¥ kiá»‡n\n",
    "\n",
    "Danh sÃ¡ch sáº£n pháº©m cÃ³ sáºµn (Ä‘á»ƒ tham kháº£o):\n",
    "{', '.join(self.available_products[:10])}...\n",
    "\n",
    "Tráº£ vá» JSON vá»›i format:\n",
    "{{\n",
    "  \"products\": [\"product_name1\", \"product_name2\"],\n",
    "  \"aspects\": [\"aspect1\", \"aspect2\"],\n",
    "  \"sentiment_focus\": \"positive|negative|null\",\n",
    "  \"is_comparison\": true|false\n",
    "}}\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"CÃ¢u há»i: {query}\\n\\nPhÃ¢n tÃ­ch:\"}\n",
    "                ],\n",
    "                temperature=0.1,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            \n",
    "            parsed = json.loads(response.choices[0].message.content)\n",
    "            print(f\"ðŸ” Parsed: {parsed}\")\n",
    "            return parsed\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Parse error: {e}\")\n",
    "            return {\n",
    "                \"products\": [],\n",
    "                \"aspects\": [\"general\"],\n",
    "                \"sentiment_focus\": None,\n",
    "                \"is_comparison\": False\n",
    "            }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODULE 3: DATA FILTER\n",
    "# ============================================================================\n",
    "\n",
    "class DataFilter:\n",
    "    \"\"\"Filter data based on parsed query\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "        self.all_products = df['product'].unique().tolist()\n",
    "    \n",
    "    def keyword_match_product(self, query_product: str) -> List[str]:\n",
    "        \"\"\"Match products using keyword extraction with exact priority\"\"\"\n",
    "        query_lower = query_product.lower().strip()\n",
    "        \n",
    "        # Extract keywords from query (split and clean)\n",
    "        keywords = []\n",
    "        for word in query_lower.split():\n",
    "            # Remove common words and keep meaningful parts\n",
    "            if len(word) >= 2 and word not in ['Ä‘iá»‡n', 'thoáº¡i', 'mÃ¡y', 'chiáº¿c', 'cÃ¡i', 'gb', 'tb']:\n",
    "                keywords.append(word)\n",
    "        \n",
    "        if not keywords:\n",
    "            return []\n",
    "        \n",
    "        exact_matches = []\n",
    "        partial_matches = []\n",
    "        \n",
    "        for product in self.all_products:\n",
    "            product_lower = product.lower()\n",
    "            \n",
    "            # Strategy 1: Exact phrase match (highest priority)\n",
    "            # Check if query is a substring of product name\n",
    "            if query_lower in product_lower:\n",
    "                # But make sure it's a word boundary match\n",
    "                # e.g., \"iphone 17\" should not match \"iphone 170\"\n",
    "                import re\n",
    "                # Add word boundaries\n",
    "                pattern = r'\\b' + re.escape(query_lower) + r'\\b'\n",
    "                if re.search(pattern, product_lower):\n",
    "                    exact_matches.append(product)\n",
    "                    continue\n",
    "            \n",
    "            # Strategy 2: All keywords must exist (partial match)\n",
    "            if all(kw in product_lower for kw in keywords):\n",
    "                # Additional check: avoid over-matching\n",
    "                # If query is \"iphone 17\", don't match \"iphone 17 pro\"\n",
    "                # unless query also has \"pro\"\n",
    "                \n",
    "                # Extract product keywords (significant words)\n",
    "                product_words = set(re.findall(r'\\b\\w+\\b', product_lower))\n",
    "                query_words = set(keywords)\n",
    "                \n",
    "                # If product has extra significant model keywords not in query, skip\n",
    "                significant_extras = ['pro', 'max', 'plus', 'ultra', 'mini', 'lite', 'note']\n",
    "                extra_in_product = product_words.intersection(significant_extras)\n",
    "                extra_in_query = query_words.intersection(significant_extras)\n",
    "                \n",
    "                # If product has model variant keywords that query doesn't have, skip it\n",
    "                if extra_in_product - extra_in_query:\n",
    "                    continue\n",
    "                \n",
    "                partial_matches.append(product)\n",
    "        \n",
    "        # Return exact matches first, then partial if no exact\n",
    "        if exact_matches:\n",
    "            return exact_matches\n",
    "        return partial_matches\n",
    "    \n",
    "    def filter_data(self, products: List[str], aspects: List[str]) -> tuple:\n",
    "        \"\"\"Filter dataframe based on products and aspects\"\"\"\n",
    "        \n",
    "        # Step 1: Match products using keywords\n",
    "        matched_products = []\n",
    "        \n",
    "        if not products:\n",
    "            # No product specified, use all\n",
    "            matched_products = self.all_products\n",
    "        else:\n",
    "            for query_prod in products:\n",
    "                matched = self.keyword_match_product(query_prod)\n",
    "                matched_products.extend(matched)\n",
    "            \n",
    "            matched_products = list(set(matched_products))\n",
    "        \n",
    "        if not matched_products:\n",
    "            print(\"âš ï¸ No products matched, using all products\")\n",
    "            matched_products = self.all_products\n",
    "        else:\n",
    "            print(f\"âœ… Matched {len(matched_products)} products: {matched_products[:3]}{'...' if len(matched_products) > 3 else ''}\")\n",
    "        \n",
    "        # Filter by products FIRST (don't filter by aspect yet for counting)\n",
    "        product_filtered = self.df[self.df['product'].isin(matched_products)].copy()\n",
    "        \n",
    "        # Count total reviews per product BEFORE aspect filtering\n",
    "        product_review_counts = {}\n",
    "        for product in matched_products:\n",
    "            product_df = product_filtered[product_filtered['product'] == product]\n",
    "            # Use review_id if available, otherwise count unique reviews\n",
    "            if 'review_id' in product_df.columns:\n",
    "                product_review_counts[product] = product_df['review_id'].nunique()\n",
    "            else:\n",
    "                product_review_counts[product] = product_df['review'].nunique()\n",
    "        \n",
    "        # Step 2: Filter by aspects for retrieval (but keep counts from above)\n",
    "        if aspects and aspects != ['general']:\n",
    "            def has_aspect_match(review_aspects):\n",
    "                if not isinstance(review_aspects, list):\n",
    "                    return False\n",
    "                return any(asp in aspects for asp in review_aspects)\n",
    "            \n",
    "            aspect_filtered = product_filtered[product_filtered['aspects'].apply(has_aspect_match)].copy()\n",
    "        else:\n",
    "            aspect_filtered = product_filtered.copy()\n",
    "        \n",
    "        print(f\"ðŸ“Š Filtered to {len(aspect_filtered)} review entries\")\n",
    "        \n",
    "        return aspect_filtered, matched_products, product_review_counts\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODULE 4: STATISTICS COMPUTER\n",
    "# ============================================================================\n",
    "\n",
    "class StatisticsComputer:\n",
    "    \"\"\"Compute statistics on filtered data\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_stats(df: pd.DataFrame, aspects: List[str], product_review_counts: Dict[str, int]) -> Dict:\n",
    "        \"\"\"Compute comprehensive statistics\n",
    "        \n",
    "        Args:\n",
    "            df: Filtered dataframe (may be filtered by aspect)\n",
    "            aspects: List of aspects to analyze\n",
    "            product_review_counts: Dict of {product: total_review_count} BEFORE aspect filtering\n",
    "        \"\"\"\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            return {\n",
    "                'total_reviews': 0,\n",
    "                'message': 'KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u phÃ¹ há»£p'\n",
    "            }\n",
    "        \n",
    "        # Total reviews across all matched products (from pre-computed counts)\n",
    "        total_reviews = sum(product_review_counts.values())\n",
    "        \n",
    "        stats = {\n",
    "            'total_reviews': total_reviews,\n",
    "            'products': {},\n",
    "            'aspects': {},\n",
    "            'overall_sentiment': {}\n",
    "        }\n",
    "        \n",
    "        # Overall sentiment distribution (count from filtered df, but calculate % based on total)\n",
    "        all_sentiments = []\n",
    "        for sentiments_dict in df['sentiments']:\n",
    "            if isinstance(sentiments_dict, dict):\n",
    "                all_sentiments.extend(sentiments_dict.values())\n",
    "        \n",
    "        sentiment_counts = Counter(all_sentiments)\n",
    "        \n",
    "        stats['overall_sentiment'] = {\n",
    "            'positive': sentiment_counts.get('Positive', 0),\n",
    "            'neutral': sentiment_counts.get('Neutral', 0),\n",
    "            'negative': sentiment_counts.get('Negative', 0),\n",
    "            'positive_pct': sentiment_counts.get('Positive', 0) / total_reviews * 100 if total_reviews > 0 else 0,\n",
    "            'negative_pct': sentiment_counts.get('Negative', 0) / total_reviews * 100 if total_reviews > 0 else 0,\n",
    "            'neutral_pct': sentiment_counts.get('Neutral', 0) / total_reviews * 100 if total_reviews > 0 else 0\n",
    "        }\n",
    "        \n",
    "        # Per-product stats\n",
    "        for product, product_total_reviews in product_review_counts.items():\n",
    "            product_df = df[df['product'] == product]\n",
    "            stats['products'][product] = StatisticsComputer._compute_product_stats(\n",
    "                product_df, \n",
    "                product_total_reviews\n",
    "            )\n",
    "        \n",
    "        # Per-aspect stats\n",
    "        for aspect in aspects:\n",
    "            aspect_reviews = []\n",
    "            aspect_sentiments = []\n",
    "            \n",
    "            for idx, row in df.iterrows():\n",
    "                if isinstance(row['aspects'], list) and aspect in row['aspects']:\n",
    "                    aspect_reviews.append(row['sentence'])\n",
    "                    if isinstance(row['sentiments'], dict) and aspect in row['sentiments']:\n",
    "                        aspect_sentiments.append(row['sentiments'][aspect])\n",
    "            \n",
    "            sentiment_counts = Counter(aspect_sentiments)\n",
    "            positive_count = sentiment_counts.get('Positive', 0)\n",
    "            negative_count = sentiment_counts.get('Negative', 0)\n",
    "            neutral_count = sentiment_counts.get('Neutral', 0)\n",
    "            \n",
    "            # Calculate based on TOTAL reviews\n",
    "            implicit_neutral = total_reviews - len(aspect_reviews)\n",
    "            total_neutral = neutral_count + implicit_neutral\n",
    "            \n",
    "            stats['aspects'][aspect] = {\n",
    "                'count': len(aspect_reviews),\n",
    "                'positive': positive_count,\n",
    "                'neutral': total_neutral,\n",
    "                'negative': negative_count,\n",
    "                'positive_pct': positive_count / total_reviews * 100 if total_reviews > 0 else 0,\n",
    "                'negative_pct': negative_count / total_reviews * 100 if total_reviews > 0 else 0,\n",
    "                'neutral_pct': total_neutral / total_reviews * 100 if total_reviews > 0 else 0,\n",
    "                'mentioned_pct': len(aspect_reviews) / total_reviews * 100 if total_reviews > 0 else 0,\n",
    "                'sample_reviews': aspect_reviews[:3]\n",
    "            }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    @staticmethod\n",
    "    def _compute_product_stats(product_df: pd.DataFrame, total_reviews: int) -> Dict:\n",
    "        \"\"\"Compute stats for a single product\n",
    "        \n",
    "        Args:\n",
    "            product_df: Filtered dataframe for this product (may be aspect-filtered)\n",
    "            total_reviews: ACTUAL total reviews for this product (before aspect filtering)\n",
    "        \"\"\"\n",
    "        all_sentiments = []\n",
    "        for sentiments_dict in product_df['sentiments']:\n",
    "            if isinstance(sentiments_dict, dict):\n",
    "                all_sentiments.extend(sentiments_dict.values())\n",
    "        \n",
    "        sentiment_counts = Counter(all_sentiments)\n",
    "        \n",
    "        return {\n",
    "            'review_count': total_reviews,\n",
    "            'positive': sentiment_counts.get('Positive', 0),\n",
    "            'neutral': sentiment_counts.get('Neutral', 0),\n",
    "            'negative': sentiment_counts.get('Negative', 0),\n",
    "            'positive_pct': sentiment_counts.get('Positive', 0) / total_reviews * 100 if total_reviews > 0 else 0,\n",
    "            'negative_pct': sentiment_counts.get('Negative', 0) / total_reviews * 100 if total_reviews > 0 else 0,\n",
    "            'neutral_pct': sentiment_counts.get('Neutral', 0) / total_reviews * 100 if total_reviews > 0 else 0\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODULE 5: VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "class ChartGenerator:\n",
    "    \"\"\"Generate visualization charts from statistics\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_sentiment_pie(stats: Dict, title: str = \"PhÃ¢n bá»‘ cáº£m xÃºc\") -> str:\n",
    "        \"\"\"Create pie chart for sentiment distribution\"\"\"\n",
    "        if 'overall_sentiment' not in stats:\n",
    "            return None\n",
    "        \n",
    "        s = stats['overall_sentiment']\n",
    "        labels = ['TÃ­ch cá»±c', 'Trung láº­p', 'TiÃªu cá»±c']\n",
    "        sizes = [s['positive'], s['neutral'], s['negative']]\n",
    "        colors = ['#4CAF50', '#FFC107', '#F44336']\n",
    "        explode = (0.05, 0, 0.05)  # Explode positive and negative\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        wedges, texts, autotexts = ax.pie(\n",
    "            sizes, \n",
    "            explode=explode,\n",
    "            labels=labels, \n",
    "            colors=colors,\n",
    "            autopct='%1.1f%%',\n",
    "            startangle=90,\n",
    "            textprops={'fontsize': 12, 'weight': 'bold'}\n",
    "        )\n",
    "        \n",
    "        # Make percentage text white\n",
    "        for autotext in autotexts:\n",
    "            autotext.set_color('white')\n",
    "            autotext.set_fontsize(14)\n",
    "        \n",
    "        ax.set_title(title, fontsize=16, weight='bold', pad=20)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Convert to base64\n",
    "        buffer = BytesIO()\n",
    "        plt.savefig(buffer, format='png', dpi=100, bbox_inches='tight')\n",
    "        buffer.seek(0)\n",
    "        img_base64 = base64.b64encode(buffer.read()).decode()\n",
    "        plt.close()\n",
    "        \n",
    "        return img_base64\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_product_comparison(stats: Dict) -> str:\n",
    "        \"\"\"Create bar chart comparing products\"\"\"\n",
    "        if 'products' not in stats or len(stats['products']) < 2:\n",
    "            return None\n",
    "        \n",
    "        products = []\n",
    "        positive = []\n",
    "        negative = []\n",
    "        \n",
    "        for product, pstats in stats['products'].items():\n",
    "            products.append(product.split()[0:3])  # Shorten name\n",
    "            positive.append(pstats['positive_pct'])\n",
    "            negative.append(pstats['negative_pct'])\n",
    "        \n",
    "        # Shorten product names\n",
    "        products = [' '.join(p) for p in products]\n",
    "        \n",
    "        x = range(len(products))\n",
    "        width = 0.35\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        bars1 = ax.bar([i - width/2 for i in x], positive, width, \n",
    "                       label='TÃ­ch cá»±c', color='#4CAF50', alpha=0.8)\n",
    "        bars2 = ax.bar([i + width/2 for i in x], negative, width,\n",
    "                       label='TiÃªu cá»±c', color='#F44336', alpha=0.8)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars1:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        for bar in bars2:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "        \n",
    "        ax.set_xlabel('Sáº£n pháº©m', fontsize=12, weight='bold')\n",
    "        ax.set_ylabel('Pháº§n trÄƒm (%)', fontsize=12, weight='bold')\n",
    "        ax.set_title('So sÃ¡nh Ä‘Ã¡nh giÃ¡ giá»¯a cÃ¡c sáº£n pháº©m', fontsize=14, weight='bold', pad=20)\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(products, rotation=15, ha='right')\n",
    "        ax.legend(fontsize=11)\n",
    "        ax.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        buffer = BytesIO()\n",
    "        plt.savefig(buffer, format='png', dpi=100, bbox_inches='tight')\n",
    "        buffer.seek(0)\n",
    "        img_base64 = base64.b64encode(buffer.read()).decode()\n",
    "        plt.close()\n",
    "        \n",
    "        return img_base64\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_aspect_breakdown(stats: Dict) -> str:\n",
    "        \"\"\"Create horizontal bar chart for aspect breakdown\"\"\"\n",
    "        if 'aspects' not in stats or not stats['aspects']:\n",
    "            return None\n",
    "        \n",
    "        aspects = []\n",
    "        positive = []\n",
    "        negative = []\n",
    "        mentioned = []\n",
    "        \n",
    "        for aspect, astats in stats['aspects'].items():\n",
    "            aspects.append(aspect.upper())\n",
    "            positive.append(astats['positive_pct'])\n",
    "            negative.append(astats['negative_pct'])\n",
    "            mentioned.append(astats['mentioned_pct'])\n",
    "        \n",
    "        # Sort by mentioned percentage\n",
    "        sorted_data = sorted(zip(aspects, positive, negative, mentioned), \n",
    "                           key=lambda x: x[3], reverse=True)\n",
    "        aspects, positive, negative, mentioned = zip(*sorted_data)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # Chart 1: Sentiment breakdown\n",
    "        y_pos = range(len(aspects))\n",
    "        \n",
    "        ax1.barh(y_pos, positive, color='#4CAF50', alpha=0.8, label='TÃ­ch cá»±c')\n",
    "        ax1.barh(y_pos, [-n for n in negative], color='#F44336', alpha=0.8, label='TiÃªu cá»±c')\n",
    "        \n",
    "        ax1.set_yticks(y_pos)\n",
    "        ax1.set_yticklabels(aspects)\n",
    "        ax1.set_xlabel('Pháº§n trÄƒm (%)', fontsize=11, weight='bold')\n",
    "        ax1.set_title('Cáº£m xÃºc theo khÃ­a cáº¡nh', fontsize=13, weight='bold')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.axvline(x=0, color='black', linewidth=0.8)\n",
    "        ax1.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Chart 2: Mention percentage\n",
    "        bars = ax2.barh(y_pos, mentioned, color='#2196F3', alpha=0.8)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, (bar, val) in enumerate(zip(bars, mentioned)):\n",
    "            ax2.text(val + 1, i, f'{val:.1f}%', va='center', fontsize=9)\n",
    "        \n",
    "        ax2.set_yticks(y_pos)\n",
    "        ax2.set_yticklabels(aspects)\n",
    "        ax2.set_xlabel('Pháº§n trÄƒm Ä‘Æ°á»£c Ä‘á» cáº­p (%)', fontsize=11, weight='bold')\n",
    "        ax2.set_title('Tá»· lá»‡ Ä‘á» cáº­p khÃ­a cáº¡nh', fontsize=13, weight='bold')\n",
    "        ax2.grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        buffer = BytesIO()\n",
    "        plt.savefig(buffer, format='png', dpi=100, bbox_inches='tight')\n",
    "        buffer.seek(0)\n",
    "        img_base64 = base64.b64encode(buffer.read()).decode()\n",
    "        plt.close()\n",
    "        \n",
    "        return img_base64\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_all_charts(stats: Dict, parsed_query: Dict) -> Dict[str, str]:\n",
    "        \"\"\"Generate all relevant charts based on query type\"\"\"\n",
    "        charts = {}\n",
    "        \n",
    "        # Always create sentiment pie chart\n",
    "        pie_chart = ChartGenerator.create_sentiment_pie(stats)\n",
    "        if pie_chart:\n",
    "            charts['sentiment_pie'] = pie_chart\n",
    "        \n",
    "        # Create product comparison if multiple products\n",
    "        if len(stats.get('products', {})) > 1 or parsed_query.get('is_comparison'):\n",
    "            comparison_chart = ChartGenerator.create_product_comparison(stats)\n",
    "            if comparison_chart:\n",
    "                charts['product_comparison'] = comparison_chart\n",
    "        \n",
    "        # Create aspect breakdown if aspects analyzed\n",
    "        if stats.get('aspects') and len(stats['aspects']) > 0:\n",
    "            aspect_chart = ChartGenerator.create_aspect_breakdown(stats)\n",
    "            if aspect_chart:\n",
    "                charts['aspect_breakdown'] = aspect_chart\n",
    "        \n",
    "        return charts\n",
    "\n",
    "class ContextBuilder:\n",
    "    \"\"\"Build context for LLM from statistics\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def build_context(stats: Dict, df: pd.DataFrame, parsed_query: Dict) -> str:\n",
    "        \"\"\"Build comprehensive context\"\"\"\n",
    "        \n",
    "        if stats.get('total_reviews', 0) == 0:\n",
    "            return \"KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u Ä‘Ã¡nh giÃ¡ phÃ¹ há»£p vá»›i cÃ¢u há»i.\"\n",
    "        \n",
    "        parts = []\n",
    "        \n",
    "        # Header\n",
    "        parts.append(f\"=== Tá»”NG QUAN ===\")\n",
    "        parts.append(f\"Tá»•ng sá»‘ Ä‘Ã¡nh giÃ¡: {stats['total_reviews']}\")\n",
    "        \n",
    "        # Overall sentiment\n",
    "        if 'overall_sentiment' in stats:\n",
    "            s = stats['overall_sentiment']\n",
    "            parts.append(f\"\\nPhÃ¢n bá»‘ cáº£m xÃºc tá»•ng thá»ƒ:\")\n",
    "            parts.append(f\"  â€¢ TÃ­ch cá»±c: {s['positive_pct']:.1f}% ({s['positive']} Ä‘Ã¡nh giÃ¡)\")\n",
    "            parts.append(f\"  â€¢ Trung láº­p: {s['neutral_pct']:.1f}% ({s['neutral']} Ä‘Ã¡nh giÃ¡)\")\n",
    "            parts.append(f\"  â€¢ TiÃªu cá»±c: {s['negative_pct']:.1f}% ({s['negative']} Ä‘Ã¡nh giÃ¡)\")\n",
    "        \n",
    "        # Per-product breakdown\n",
    "        if len(stats['products']) > 1 or parsed_query.get('is_comparison'):\n",
    "            parts.append(f\"\\n=== SO SÃNH Sáº¢N PHáº¨M ===\")\n",
    "            for product, pstats in stats['products'].items():\n",
    "                parts.append(f\"\\n{product}:\")\n",
    "                parts.append(f\"  â€¢ Sá»‘ Ä‘Ã¡nh giÃ¡: {pstats['review_count']}\")\n",
    "                parts.append(f\"  â€¢ TÃ­ch cá»±c: {pstats['positive_pct']:.1f}% ({pstats['positive']})\")\n",
    "                parts.append(f\"  â€¢ TiÃªu cá»±c: {pstats['negative_pct']:.1f}% ({pstats['negative']})\")\n",
    "        \n",
    "        # Per-aspect breakdown\n",
    "        if stats['aspects']:\n",
    "            parts.append(f\"\\n=== PHÃ‚N TÃCH THEO KHÃA Cáº NH ===\")\n",
    "            for aspect, astats in stats['aspects'].items():\n",
    "                parts.append(f\"\\n{aspect.upper()}:\")\n",
    "                parts.append(f\"  â€¢ Sá»‘ Ä‘Ã¡nh giÃ¡ Ä‘á» cáº­p: {astats['count']}/{stats['total_reviews']} ({astats['mentioned_pct']:.1f}%)\")\n",
    "                parts.append(f\"  â€¢ TÃ­ch cá»±c: {astats['positive_pct']:.1f}% ({astats['positive']} Ä‘Ã¡nh giÃ¡)\")\n",
    "                parts.append(f\"  â€¢ TiÃªu cá»±c: {astats['negative_pct']:.1f}% ({astats['negative']} Ä‘Ã¡nh giÃ¡)\")\n",
    "                parts.append(f\"  â€¢ Trung láº­p/KhÃ´ng Ä‘á» cáº­p: {astats['neutral_pct']:.1f}% ({astats['neutral']} Ä‘Ã¡nh giÃ¡)\")\n",
    "                \n",
    "                if astats['sample_reviews']:\n",
    "                    parts.append(f\"  â€¢ VÃ­ dá»¥:\")\n",
    "                    for i, review in enumerate(astats['sample_reviews'][:2], 1):\n",
    "                        parts.append(f\"    {i}. {review[:150]}\")\n",
    "        \n",
    "        # Sample reviews\n",
    "        parts.append(f\"\\n=== MáºªU ÄÃNH GIÃ ===\")\n",
    "        sample_df = df.head(5)\n",
    "        for i, row in enumerate(sample_df.iterrows(), 1):\n",
    "            idx, r = row\n",
    "            parts.append(f\"\\n{i}. [{r['product']}]\")\n",
    "            parts.append(f\"   KhÃ­a cáº¡nh: {', '.join(r['aspects']) if isinstance(r['aspects'], list) else 'N/A'}\")\n",
    "            parts.append(f\"   Ná»™i dung: {r['sentence'][:200]}\")\n",
    "        \n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MODULE 7: RAG SYSTEM\n",
    "# ============================================================================\n",
    "\n",
    "class SimpleRAG:\n",
    "    \"\"\"Main RAG system\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path: str, openai_api_key: Optional[str] = None):\n",
    "        # Load API key\n",
    "        api_key = openai_api_key or os.getenv('OPENAI_API_KEY')\n",
    "        if not api_key:\n",
    "            raise ValueError(\"âŒ OpenAI API key not found! Add to .env file\")\n",
    "        \n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        \n",
    "        # Load data\n",
    "        loader = DataLoader(csv_path)\n",
    "        self.df = loader.load_data()\n",
    "        \n",
    "        # Initialize modules\n",
    "        self.parser = QueryParser(self.client, self.df['product'].unique().tolist())\n",
    "        self.filter = DataFilter(self.df)\n",
    "        \n",
    "        print(\"âœ… System ready!\")\n",
    "    \n",
    "    def answer(self, query: str, show_charts: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Main method: answer user query\n",
    "        \n",
    "        Args:\n",
    "            query: User question\n",
    "            show_charts: Whether to generate visualization charts\n",
    "            \n",
    "        Returns:\n",
    "            Dict with 'answer' and 'charts' (base64 encoded images)\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"â“ Query: {query}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Step 1: Parse query with LLM\n",
    "        print(\"\\nðŸ” Step 1: Parsing query...\")\n",
    "        parsed = self.parser.parse_query(query)\n",
    "        \n",
    "        # Step 2: Filter data\n",
    "        print(\"\\nðŸ“Š Step 2: Filtering data...\")\n",
    "        filtered_df, matched_products, product_review_counts = self.filter.filter_data(\n",
    "            parsed.get('products', []),\n",
    "            parsed.get('aspects', ['general'])\n",
    "        )\n",
    "        \n",
    "        if len(filtered_df) == 0:\n",
    "            return {\n",
    "                'answer': \"âš ï¸ KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u phÃ¹ há»£p vá»›i cÃ¢u há»i cá»§a báº¡n.\",\n",
    "                'charts': {}\n",
    "            }\n",
    "        \n",
    "        # Step 3: Compute statistics\n",
    "        print(\"\\nðŸ“ˆ Step 3: Computing statistics...\")\n",
    "        stats = StatisticsComputer.compute_stats(\n",
    "            filtered_df,\n",
    "            parsed.get('aspects', ['general']),\n",
    "            product_review_counts\n",
    "        )\n",
    "        \n",
    "        # Step 4: Build context\n",
    "        print(\"\\nðŸ“ Step 4: Building context...\")\n",
    "        context = ContextBuilder.build_context(stats, filtered_df, parsed)\n",
    "        \n",
    "        # Step 5: Generate charts\n",
    "        charts = {}\n",
    "        if show_charts:\n",
    "            print(\"\\nðŸ“Š Step 5: Generating charts...\")\n",
    "            charts = ChartGenerator.generate_all_charts(stats, parsed)\n",
    "            print(f\"âœ… Generated {len(charts)} chart(s)\")\n",
    "        \n",
    "        # Step 6: Generate answer with LLM\n",
    "        print(\"\\nðŸ¤– Step 6: Generating answer...\")\n",
    "        answer = self._generate_answer(query, context, parsed)\n",
    "        \n",
    "        return {\n",
    "            'answer': answer,\n",
    "            'charts': charts,\n",
    "            'stats': stats\n",
    "        }\n",
    "    \n",
    "    def _generate_answer(self, query: str, context: str, parsed_query: Dict) -> str:\n",
    "        \"\"\"Generate final answer using LLM\"\"\"\n",
    "        \n",
    "        system_prompt = \"\"\"Báº¡n lÃ  trá»£ lÃ½ phÃ¢n tÃ­ch Ä‘Ã¡nh giÃ¡ Ä‘iá»‡n thoáº¡i chuyÃªn nghiá»‡p.\n",
    "\n",
    "NHIá»†M Vá»¤:\n",
    "Dá»±a trÃªn dá»¯ liá»‡u thá»‘ng kÃª Ä‘Æ°á»£c cung cáº¥p, hÃ£y tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng má»™t cÃ¡ch:\n",
    "- ChÃ­nh xÃ¡c, dá»±a trÃªn sá»‘ liá»‡u cá»¥ thá»ƒ\n",
    "- CÃ¢n báº±ng, khÃ´ng thiÃªn vá»‹\n",
    "- Dá»… hiá»ƒu, sÃºc tÃ­ch\n",
    "- TrÃ­ch dáº«n % vÃ  sá»‘ lÆ°á»£ng review\n",
    "\n",
    "Cáº¤U TRÃšC TRáº¢ Lá»œI:\n",
    "1. TÃ³m táº¯t ngáº¯n gá»n (1-2 cÃ¢u)\n",
    "2. PhÃ¢n tÃ­ch sá»‘ liá»‡u chi tiáº¿t\n",
    "3. ÄÆ°a ra vÃ­ dá»¥ tá»« review (náº¿u cÃ³)\n",
    "4. Káº¿t luáº­n\n",
    "\n",
    "LÆ¯U Ã:\n",
    "- KHÃ”NG bá»‹a Ä‘áº·t thÃ´ng tin ngoÃ i context\n",
    "- Náº¿u dá»¯ liá»‡u khÃ´ng Ä‘á»§, nÃ³i rÃµ háº¡n cháº¿\n",
    "- Æ¯u tiÃªn sá»‘ liá»‡u thá»‘ng kÃª hÆ¡n review Ä‘Æ¡n láº»\"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"Dá»¯ liá»‡u thá»‘ng kÃª:\n",
    "{context}\n",
    "\n",
    "CÃ¢u há»i: {query}\n",
    "\n",
    "HÃ£y tráº£ lá»i dá»±a trÃªn dá»¯ liá»‡u trÃªn:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "            \n",
    "            answer = response.choices[0].message.content\n",
    "            \n",
    "            # Log token usage\n",
    "            usage = response.usage\n",
    "            print(f\"ðŸ’° Tokens: {usage.total_tokens} (prompt: {usage.prompt_tokens}, completion: {usage.completion_tokens})\")\n",
    "            \n",
    "            return answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"âš ï¸ Lá»—i khi generate answer: {e}\\n\\nContext:\\n{context}\"\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run the system\"\"\"\n",
    "    \n",
    "    CSV_PATH = \"clean_reviews.csv\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"ðŸš€ SIMPLE STATISTICS RAG SYSTEM WITH VISUALIZATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize RAG\n",
    "    rag = SimpleRAG(CSV_PATH)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"âœ… READY TO USE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nðŸ“ Example usage:\")\n",
    "    print(\"result = rag.answer('Pin Xiaomi 15T cÃ³ tá»‘t khÃ´ng?')\")\n",
    "    print(\"print(result['answer'])\")\n",
    "    print(\"# Save charts:\")\n",
    "    print(\"for chart_name, img_base64 in result['charts'].items():\")\n",
    "    print(\"    with open(f'{chart_name}.png', 'wb') as f:\")\n",
    "    print(\"        f.write(base64.b64decode(img_base64))\")\n",
    "    \n",
    "    return rag\n",
    "\n",
    "\n",
    "def demo():\n",
    "    \"\"\"Interactive demo with chart display\"\"\"\n",
    "    rag = main()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸŽ® INTERACTIVE DEMO\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Type 'exit' to quit\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\nðŸ’¬ Your question: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['exit', 'quit', 'q']:\n",
    "                print(\"ðŸ‘‹ Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            # Get result\n",
    "            result = rag.answer(user_input)\n",
    "            \n",
    "            # Display answer\n",
    "            print(f\"\\nðŸ¤– Answer:\\n{result['answer']}\\n\")\n",
    "            \n",
    "            # Save charts\n",
    "            if result['charts']:\n",
    "                print(f\"ðŸ“Š Generated {len(result['charts'])} chart(s):\")\n",
    "                for chart_name, img_base64 in result['charts'].items():\n",
    "                    filename = f\"{chart_name}.png\"\n",
    "                    with open(filename, 'wb') as f:\n",
    "                        f.write(base64.b64decode(img_base64))\n",
    "                    print(f\"  âœ… Saved: {filename}\")\n",
    "                print(f\"\\nðŸ’¡ Tip: Open the PNG files to view charts!\\n\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nðŸ‘‹ Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâš ï¸ Error: {e}\")\n",
    "\n",
    "\n",
    "def display_charts_html(charts: Dict[str, str], output_file: str = \"charts.html\"):\n",
    "    \"\"\"Generate HTML file to display all charts in browser\"\"\"\n",
    "    html_content = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <title>Review Analysis Charts</title>\n",
    "        <style>\n",
    "            body {\n",
    "                font-family: Arial, sans-serif;\n",
    "                max-width: 1400px;\n",
    "                margin: 0 auto;\n",
    "                padding: 20px;\n",
    "                background: #f5f5f5;\n",
    "            }\n",
    "            h1 {\n",
    "                color: #333;\n",
    "                text-align: center;\n",
    "                margin-bottom: 30px;\n",
    "            }\n",
    "            .chart-container {\n",
    "                background: white;\n",
    "                padding: 20px;\n",
    "                margin: 20px 0;\n",
    "                border-radius: 8px;\n",
    "                box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "            }\n",
    "            .chart-title {\n",
    "                color: #555;\n",
    "                font-size: 18px;\n",
    "                font-weight: bold;\n",
    "                margin-bottom: 15px;\n",
    "            }\n",
    "            img {\n",
    "                max-width: 100%;\n",
    "                height: auto;\n",
    "                display: block;\n",
    "                margin: 0 auto;\n",
    "            }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>ðŸ“Š Biá»ƒu Ä‘á»“ phÃ¢n tÃ­ch Ä‘Ã¡nh giÃ¡</h1>\n",
    "    \"\"\"\n",
    "    \n",
    "    chart_titles = {\n",
    "        'sentiment_pie': 'ðŸ¥§ PhÃ¢n bá»‘ cáº£m xÃºc tá»•ng thá»ƒ',\n",
    "        'product_comparison': 'ðŸ“± So sÃ¡nh sáº£n pháº©m',\n",
    "        'aspect_breakdown': 'ðŸ” PhÃ¢n tÃ­ch theo khÃ­a cáº¡nh'\n",
    "    }\n",
    "    \n",
    "    for chart_name, img_base64 in charts.items():\n",
    "        title = chart_titles.get(chart_name, chart_name)\n",
    "        html_content += f\"\"\"\n",
    "        <div class=\"chart-container\">\n",
    "            <div class=\"chart-title\">{title}</div>\n",
    "            <img src=\"data:image/png;base64,{img_base64}\" alt=\"{chart_name}\">\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    html_content += \"\"\"\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    print(f\"ðŸ“„ HTML report saved: {output_file}\")\n",
    "    print(f\"ðŸ’¡ Open {output_file} in your browser to view charts!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run setup\n",
    "    rag = main()\n",
    "    \n",
    "    # Example: Run a query and display charts\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ§ª RUNNING EXAMPLE QUERY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    result = rag.answer(\"So sÃ¡nh pin Xiaomi 15T vÃ  15T Pro\")\n",
    "    print(f\"\\nðŸ¤– Answer:\\n{result['answer']}\\n\")\n",
    "    \n",
    "    if result['charts']:\n",
    "        # Generate HTML report\n",
    "        display_charts_html(result['charts'], \"review_analysis.html\")\n",
    "    \n",
    "    # Uncomment to run interactive demo\n",
    "    demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VSF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
